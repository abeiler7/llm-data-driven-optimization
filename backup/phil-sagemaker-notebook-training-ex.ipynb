{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LLaMA 2 on Amazon SageMaker\n",
    "\n",
    "In this sagemaker example, we are going to learn how to fine-tune [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) using [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314). [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) is the next version of the [LLaMA](https://arxiv.org/abs/2302.13971). Compared to the V1 model, it is trained on more data - 2T tokens and supports context length window upto 4K tokens. Learn more about LLaMa 2 in the [\"\"]() blog post.\n",
    "\n",
    "QLoRA is an efficient finetuning technique that quantizes a pretrained language model to 4 bits and attaches small ‚ÄúLow-Rank Adapters‚Äù which are fine-tuned. This enables fine-tuning of models with up to 65 billion parameters on a single GPU; despite its efficiency, QLoRA matches the performance of full-precision fine-tuning and achieves state-of-the-art results on language tasks.\n",
    "\n",
    "In our example, we are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). \n",
    "\n",
    "In Detail you will learn how to:\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "4. Deploy Fine-tuned LLM on Amazon SageMaker\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
    "\n",
    "- (Q)LoRA:¬†[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning:¬†[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning:¬†[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning:¬†[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "- IA3: [Infused Adapter by Inhibiting and Amplifying Inner Activations](https://arxiv.org/abs/2205.05638)\n",
    "\n",
    "\n",
    "\n",
    "### Access LLaMA 2\n",
    "\n",
    "Before we can start training we have to make sure that we accepted the license of [llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) to be able to use it. You can accept the license by clicking on the Agree and access repository button on the model page at: \n",
    "* [LLaMa 7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "* [LLaMa 13B](https://huggingface.co/meta-llama/Llama-2-13b-hf)\n",
    "* [LLaMa 70B](https://huggingface.co/meta-llama/Llama-2-70b-hf)\n",
    "\n",
    "## 1. Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_PROFILE=dev-admin\n",
      "env: AWS_REGION=us-east-1\n",
      "env: HF_HOME=~/.cache/huggingface\n",
      "env: TOKENIZERS_PARALLELISM=fale\n"
     ]
    }
   ],
   "source": [
    "%env AWS_PROFILE=dev-admin\n",
    "%env AWS_REGION=us-east-1\n",
    "%env HF_HOME=~/.cache/huggingface\n",
    "%env TOKENIZERS_PARALLELISM=fale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access any LLaMA 2 asset we need to login into our hugging face account. We can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token YOUR_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker bucket: sagemaker-ms-thesis-llm\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "from scripts.aws_init import init_sagemaker\n",
    "\n",
    "sagemaker_session_bucket = \"sagemaker-ms-thesis-llm\"\n",
    "# role = \"arn:aws:iam::171706357329:role/service-role/SageMaker-ComputeAdmin\"\n",
    "role = \"arn:aws:iam::171706357329:role/service-role/AmazonSageMakerServiceCatalogProductsExecutionRole\"\n",
    "\n",
    "sess = init_sagemaker(sagemaker_session_bucket)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "we will use the¬†[dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)¬†an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the¬†[InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load the¬†`samsum`¬†dataset, we use the¬†`load_dataset()`¬†method from the ü§ó Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewbeiler/projects/llm-data-driven-optimization/venv_310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.20k/8.20k [00:00<00:00, 7.17MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/databricks--databricks-dolly-15k to /Users/andrewbeiler/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13.1M/13.1M [00:01<00:00, 10.2MB/s]\n",
      "Downloading data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.22s/it]\n",
      "Extracting data files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 593.09it/s]\n",
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/andrewbeiler/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
      "dataset size: 15011\n",
      "{'instruction': \"Who was Manchester United's most successful manager?\", 'context': \"Manchester United have won a record 20 League titles, 12 FA Cups, six League Cups, and a record 21 FA Community Shields. They have won the European Cup/UEFA Champions League three times, and the UEFA Europa League, the UEFA Cup Winners' Cup, the UEFA Super Cup, the Intercontinental Cup and the FIFA Club World Cup once each. In 1968, under the management of Matt Busby, 10 years after eight of the club's players were killed in the Munich air disaster, they became the first English club to win the European Cup. Sir Alex Ferguson is the club's longest-serving and most successful manager, winning 38 trophies, including 13 league titles, five FA Cups, and two Champions League titles between 1986 and 2013. In the 1998‚Äì99 season, under Ferguson, the club became the first in the history of English football to achieve the European treble of the Premier League, FA Cup, and UEFA Champions League. In winning the UEFA Europa League under Jos√© Mourinho in 2016‚Äì17, they became one of five clubs to have won the original three main UEFA club competitions (the Champions League, Europa League and Cup Winners' Cup).\", 'response': \"Manchester United's most successful manager is Sir Alex Ferguson who won 38 trophies including 13 league titles\", 'category': 'summarization'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[1;32m      3\u001b[0m n_samples \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\u001b[39m#0000\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m n_shards \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mceil(\u001b[39mlen\u001b[39m(dataset)\u001b[39m/\u001b[39mn_samples)\n\u001b[1;32m      5\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mshard(n_shards,\u001b[39m2\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdataset_sample size: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(dataset)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n_samples = 100#0000\n",
    "n_shards = math.ceil(len(dataset)/n_samples)\n",
    "dataset = dataset.shard(n_shards,2)\n",
    "print(f\"dataset_sample size: {len(dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "What is a romance language?\n",
      "\n",
      "### Answer\n",
      "Romance languages are a subset of languages derived from Latin roots into fully fledged nationally spoken languages. Examples of Romance languages are French, Italian, Spanish, and Portuguese.\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewbeiler/projects/llm-data-driven-optimization/venv_310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some helper functions to pack our samples into sequences of a given length and then tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Classify each of the following as either round or square shaped: a planet, a a ball, a slice of bread, a chess board.\n",
      "\n",
      "### Answer\n",
      "Planets are round.\n",
      "Balls are round.\n",
      "A slice of bread is square shaped.\n",
      "A chess board is square shaped.</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-ms-thesis-llm/dataseta/dolly/v1/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "ver = \"v1\"\n",
    "training_input_path = f's3://{sess.default_bucket()}/dataseta/dolly/{ver}/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is: \n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code. The model will be temporally offloaded to disk, if it is too large to fit into memory.\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n",
    "### Harwarde requirements\n",
    "\n",
    "We also ran several experiments to determine, which instance type can be used for the different model sizes. The following table shows the results of our experiments. The table shows the instance type, model size, context length, and max batch size. \n",
    "\n",
    "| Model        | Instance Type     | Max Batch Size | Context Length |\n",
    "|--------------|-------------------|----------------|----------------|\n",
    "| [LLama 7B]() | `(ml.)g5.4xlarge` | `3`            | `2048`         |\n",
    "| [LLama 13B]() | `(ml.)g5.4xlarge` | `2`            | `2048`         |\n",
    "| [LLama 70B]() | `(ml.)p4d.24xlarge` | `1++` (need to test more configs)            | `2048`         |\n",
    "\n",
    "\n",
    "> You can also use `g5.2xlarge` instead of the `g5.4xlarge` instance type, but then it is not possible to use `merge_weights` parameter, since to merge the LoRA weights into the model weights, the model needs to fit into memory. But you could save the adapter weights and merge them using [merge_adapter_weights.py](./scripts/merge_adapter_weights.py) after training.\n",
    "\n",
    "_Note: We plan to extend this list in the future. feel free to contribute your setup!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-dolly-example'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 1,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'phil-examples',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-dolly-example-2023-08-21-14-08-50-477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-21 14:08:51 Starting - Starting the training job...\n",
      "2023-08-21 14:09:07 Starting - Preparing the instances for training......\n",
      "2023-08-21 14:10:29 Downloading - Downloading input data\n",
      "2023-08-21 14:10:29 Training - Downloading the training image...........................\n",
      "2023-08-21 14:15:01 Training - Training image download completed. Training in progress......bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-08-21 14:15:58,889 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-08-21 14:15:58,902 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-08-21 14:15:58,911 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-08-21 14:15:58,913 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-08-21 14:16:00,218 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.10 -m pip install -r requirements.txt\n",
      "Collecting transformers==4.31.0 (from -r requirements.txt (line 1))\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7.4/7.4 MB 76.8 MB/s eta 0:00:00\n",
      "Collecting peft==0.4.0 (from -r requirements.txt (line 2))\n",
      "Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 72.9/72.9 kB 21.9 MB/s eta 0:00:00\n",
      "Collecting accelerate==0.21.0 (from -r requirements.txt (line 3))\n",
      "Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 244.2/244.2 kB 46.5 MB/s eta 0:00:00\n",
      "Collecting bitsandbytes==0.40.2 (from -r requirements.txt (line 4))\n",
      "Downloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 92.5/92.5 MB 27.0 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from -r requirements.txt (line 5))\n",
      "Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.3/1.3 MB 92.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tokenizers>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.13.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (1.3.0)\n",
      "Installing collected packages: safetensors, bitsandbytes, transformers, accelerate, peft\n",
      "Attempting uninstall: transformers\n",
      "Found existing installation: transformers 4.28.1\n",
      "Uninstalling transformers-4.28.1:\n",
      "Successfully uninstalled transformers-4.28.1\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.19.0\n",
      "Uninstalling accelerate-0.19.0:\n",
      "Successfully uninstalled accelerate-0.19.0\n",
      "Successfully installed accelerate-0.21.0 bitsandbytes-0.40.2 peft-0.4.0 safetensors-0.3.2 transformers-4.31.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2023-08-21 14:16:10,738 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-08-21 14:16:10,738 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-08-21 14:16:10,773 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-08-21 14:16:10,796 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-08-21 14:16:10,819 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-08-21 14:16:10,830 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"epochs\": 1,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"lr\": 0.0002,\n",
      "        \"merge_weights\": true,\n",
      "        \"model_id\": \"meta-llama/Llama-2-7b-hf\",\n",
      "        \"per_device_train_batch_size\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-qlora-dolly-example-2023-08-21-14-08-50-477\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-171706357329/huggingface-qlora-dolly-example-2023-08-21-14-08-50-477/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"hf_token\":\"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\",\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"meta-llama/Llama-2-7b-hf\",\"per_device_train_batch_size\":2}\n",
      "SM_USER_ENTRY_POINT=run_clm.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"training\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run_clm\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=16\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-171706357329/huggingface-qlora-dolly-example-2023-08-21-14-08-50-477/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_path\":\"/opt/ml/input/data/training\",\"epochs\":1,\"hf_token\":\"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\",\"lr\":0.0002,\"merge_weights\":true,\"model_id\":\"meta-llama/Llama-2-7b-hf\",\"per_device_train_batch_size\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-qlora-dolly-example-2023-08-21-14-08-50-477\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-171706357329/huggingface-qlora-dolly-example-2023-08-21-14-08-50-477/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\n",
      "SM_USER_ARGS=[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--epochs\",\"1\",\"--hf_token\",\"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\",\"--lr\",\"0.0002\",\"--merge_weights\",\"True\",\"--model_id\",\"meta-llama/Llama-2-7b-hf\",\"--per_device_train_batch_size\",\"2\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "SM_HP_DATASET_PATH=/opt/ml/input/data/training\n",
      "SM_HP_EPOCHS=1\n",
      "SM_HP_HF_TOKEN=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\n",
      "SM_HP_LR=0.0002\n",
      "SM_HP_MERGE_WEIGHTS=true\n",
      "SM_HP_MODEL_ID=meta-llama/Llama-2-7b-hf\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.10 run_clm.py --dataset_path /opt/ml/input/data/training --epochs 1 --hf_token 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 --lr 0.0002 --merge_weights True --model_id meta-llama/Llama-2-7b-hf --per_device_train_batch_size 2\n",
      "2023-08-21 14:16:10,856 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Logging into the Hugging Face Hub with token hf_weneUFv...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]\n",
      "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 609/609 [00:00<00:00, 4.61MB/s]\n",
      "Downloading (‚Ä¶)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]\n",
      "Downloading (‚Ä¶)fetensors.index.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.8k/26.8k [00:00<00:00, 192MB/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]\n",
      "#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   1%|          | 52.4M/9.98G [00:00<00:23, 431MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   1%|          | 105M/9.98G [00:00<00:21, 454MB/s] #033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   2%|‚ñè         | 157M/9.98G [00:00<00:21, 463MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   2%|‚ñè         | 210M/9.98G [00:00<00:20, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   3%|‚ñé         | 262M/9.98G [00:00<00:20, 472MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   3%|‚ñé         | 315M/9.98G [00:00<00:20, 475MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   4%|‚ñé         | 367M/9.98G [00:00<00:20, 474MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   4%|‚ñç         | 419M/9.98G [00:00<00:20, 474MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   5%|‚ñç         | 472M/9.98G [00:01<00:23, 399MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   5%|‚ñå         | 514M/9.98G [00:01<00:24, 388MB/s]\n",
      "#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   6%|‚ñå         | 566M/9.98G [00:01<00:22, 409MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   6%|‚ñå         | 619M/9.98G [00:01<00:21, 427MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   7%|‚ñã         | 671M/9.98G [00:01<00:22, 423MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   7%|‚ñã         | 724M/9.98G [00:01<00:21, 425MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   8%|‚ñä         | 776M/9.98G [00:01<00:20, 440MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   8%|‚ñä         | 828M/9.98G [00:01<00:20, 455MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   9%|‚ñâ         | 881M/9.98G [00:01<00:19, 458MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   9%|‚ñâ         | 933M/9.98G [00:02<00:19, 462MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  10%|‚ñâ         | 986M/9.98G [00:02<00:19, 467MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  10%|‚ñà         | 1.04G/9.98G [00:02<00:19, 451MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  11%|‚ñà         | 1.09G/9.98G [00:02<00:19, 445MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  11%|‚ñà‚ñè        | 1.14G/9.98G [00:02<00:19, 452MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  12%|‚ñà‚ñè        | 1.20G/9.98G [00:02<00:19, 459MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  13%|‚ñà‚ñé        | 1.25G/9.98G [00:02<00:19, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  13%|‚ñà‚ñé        | 1.30G/9.98G [00:02<00:19, 456MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  14%|‚ñà‚ñé        | 1.35G/9.98G [00:03<00:18, 459MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  14%|‚ñà‚ñç        | 1.41G/9.98G [00:03<00:18, 464MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  15%|‚ñà‚ñç        | 1.46G/9.98G [00:03<00:18, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  15%|‚ñà‚ñå        | 1.51G/9.98G [00:03<00:18, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  16%|‚ñà‚ñå        | 1.56G/9.98G [00:03<00:17, 471MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  16%|‚ñà‚ñå        | 1.61G/9.98G [00:03<00:18, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  17%|‚ñà‚ñã        | 1.67G/9.98G [00:03<00:19, 437MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  17%|‚ñà‚ñã        | 1.72G/9.98G [00:03<00:22, 371MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  18%|‚ñà‚ñä        | 1.77G/9.98G [00:04<00:21, 387MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  18%|‚ñà‚ñä        | 1.82G/9.98G [00:04<00:20, 405MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  19%|‚ñà‚ñä        | 1.87G/9.98G [00:04<00:20, 401MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  19%|‚ñà‚ñâ        | 1.92G/9.98G [00:04<00:19, 420MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  20%|‚ñà‚ñâ        | 1.97G/9.98G [00:04<00:18, 433MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  20%|‚ñà‚ñà        | 2.02G/9.98G [00:04<00:19, 411MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  21%|‚ñà‚ñà        | 2.08G/9.98G [00:04<00:18, 429MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  21%|‚ñà‚ñà‚ñè       | 2.13G/9.98G [00:04<00:19, 404MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  22%|‚ñà‚ñà‚ñè       | 2.18G/9.98G [00:04<00:18, 418MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  22%|‚ñà‚ñà‚ñè       | 2.23G/9.98G [00:05<00:17, 433MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  23%|‚ñà‚ñà‚ñé       | 2.29G/9.98G [00:05<00:17, 443MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  23%|‚ñà‚ñà‚ñé       | 2.34G/9.98G [00:05<00:16, 450MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  24%|‚ñà‚ñà‚ñç       | 2.39G/9.98G [00:05<00:16, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  24%|‚ñà‚ñà‚ñç       | 2.44G/9.98G [00:05<00:16, 461MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  25%|‚ñà‚ñà‚ñå       | 2.50G/9.98G [00:05<00:16, 446MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  26%|‚ñà‚ñà‚ñå       | 2.55G/9.98G [00:05<00:16, 450MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  26%|‚ñà‚ñà‚ñå       | 2.60G/9.98G [00:05<00:16, 456MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  27%|‚ñà‚ñà‚ñã       | 2.65G/9.98G [00:06<00:15, 460MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  27%|‚ñà‚ñà‚ñã       | 2.71G/9.98G [00:06<00:15, 462MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  28%|‚ñà‚ñà‚ñä       | 2.76G/9.98G [00:06<00:15, 465MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  28%|‚ñà‚ñà‚ñä       | 2.81G/9.98G [00:06<00:15, 465MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  29%|‚ñà‚ñà‚ñä       | 2.86G/9.98G [00:06<00:15, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  29%|‚ñà‚ñà‚ñâ       | 2.92G/9.98G [00:06<00:15, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  30%|‚ñà‚ñà‚ñâ       | 2.97G/9.98G [00:06<00:15, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  30%|‚ñà‚ñà‚ñà       | 3.02G/9.98G [00:06<00:14, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  31%|‚ñà‚ñà‚ñà       | 3.07G/9.98G [00:06<00:14, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  31%|‚ñà‚ñà‚ñà‚ñè      | 3.12G/9.98G [00:07<00:14, 465MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  32%|‚ñà‚ñà‚ñà‚ñè      | 3.18G/9.98G [00:07<00:14, 468MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  32%|‚ñà‚ñà‚ñà‚ñè      | 3.23G/9.98G [00:07<00:14, 468MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  33%|‚ñà‚ñà‚ñà‚ñé      | 3.28G/9.98G [00:07<00:14, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  33%|‚ñà‚ñà‚ñà‚ñé      | 3.33G/9.98G [00:07<00:14, 472MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  34%|‚ñà‚ñà‚ñà‚ñç      | 3.39G/9.98G [00:07<00:14, 459MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  34%|‚ñà‚ñà‚ñà‚ñç      | 3.44G/9.98G [00:07<00:14, 459MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  35%|‚ñà‚ñà‚ñà‚ñç      | 3.49G/9.98G [00:07<00:13, 465MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  36%|‚ñà‚ñà‚ñà‚ñå      | 3.54G/9.98G [00:07<00:13, 468MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  36%|‚ñà‚ñà‚ñà‚ñå      | 3.60G/9.98G [00:08<00:13, 471MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  37%|‚ñà‚ñà‚ñà‚ñã      | 3.65G/9.98G [00:08<00:13, 472MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  37%|‚ñà‚ñà‚ñà‚ñã      | 3.70G/9.98G [00:08<00:13, 471MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  38%|‚ñà‚ñà‚ñà‚ñä      | 3.75G/9.98G [00:08<00:13, 465MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  38%|‚ñà‚ñà‚ñà‚ñä      | 3.81G/9.98G [00:08<00:13, 461MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  39%|‚ñà‚ñà‚ñà‚ñä      | 3.86G/9.98G [00:08<00:13, 465MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  39%|‚ñà‚ñà‚ñà‚ñâ      | 3.91G/9.98G [00:08<00:13, 463MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  40%|‚ñà‚ñà‚ñà‚ñâ      | 3.96G/9.98G [00:08<00:12, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  40%|‚ñà‚ñà‚ñà‚ñà      | 4.02G/9.98G [00:08<00:12, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  41%|‚ñà‚ñà‚ñà‚ñà      | 4.07G/9.98G [00:09<00:12, 470MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 4.12G/9.98G [00:09<00:12, 456MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 4.17G/9.98G [00:09<00:12, 462MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 4.23G/9.98G [00:09<00:12, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 4.28G/9.98G [00:09<00:12, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 4.33G/9.98G [00:09<00:11, 472MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4.38G/9.98G [00:09<00:11, 470MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4.44G/9.98G [00:09<00:11, 472MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4.49G/9.98G [00:09<00:11, 474MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 4.54G/9.98G [00:10<00:11, 473MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 4.59G/9.98G [00:10<00:11, 474MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 4.65G/9.98G [00:10<00:11, 473MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 4.70G/9.98G [00:10<00:11, 474MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 4.75G/9.98G [00:10<00:11, 449MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 4.80G/9.98G [00:10<00:11, 450MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 4.85G/9.98G [00:10<00:11, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4.91G/9.98G [00:10<00:10, 462MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4.96G/9.98G [00:10<00:10, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5.01G/9.98G [00:11<00:16, 304MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5.06G/9.98G [00:11<00:14, 340MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 5.12G/9.98G [00:11<00:13, 373MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 5.17G/9.98G [00:11<00:12, 399MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 5.22G/9.98G [00:11<00:11, 419MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 5.27G/9.98G [00:11<00:10, 435MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 5.33G/9.98G [00:11<00:10, 441MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 5.38G/9.98G [00:12<00:10, 451MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 5.43G/9.98G [00:12<00:09, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 5.48G/9.98G [00:12<00:09, 461MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5.54G/9.98G [00:12<00:09, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5.59G/9.98G [00:12<00:09, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 5.64G/9.98G [00:12<00:09, 471MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 5.69G/9.98G [00:12<00:11, 358MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 5.75G/9.98G [00:12<00:11, 381MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 5.80G/9.98G [00:13<00:10, 400MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 5.85G/9.98G [00:13<00:09, 419MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 5.90G/9.98G [00:13<00:09, 434MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 5.96G/9.98G [00:13<00:09, 445MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6.01G/9.98G [00:13<00:08, 452MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6.06G/9.98G [00:13<00:08, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 6.11G/9.98G [00:13<00:08, 459MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 6.17G/9.98G [00:13<00:08, 461MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 6.22G/9.98G [00:13<00:08, 445MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 6.27G/9.98G [00:14<00:08, 442MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 6.32G/9.98G [00:14<00:08, 445MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6.38G/9.98G [00:14<00:08, 425MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6.43G/9.98G [00:14<00:08, 437MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 6.48G/9.98G [00:14<00:07, 445MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 6.53G/9.98G [00:14<00:07, 439MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 6.59G/9.98G [00:14<00:07, 447MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6.64G/9.98G [00:14<00:07, 452MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6.69G/9.98G [00:15<00:07, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 6.74G/9.98G [00:15<00:07, 459MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 6.79G/9.98G [00:15<00:06, 464MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 6.85G/9.98G [00:15<00:06, 467MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 6.90G/9.98G [00:15<00:06, 470MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 6.95G/9.98G [00:15<00:06, 474MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7.00G/9.98G [00:15<00:06, 474MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7.06G/9.98G [00:15<00:06, 474MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 7.11G/9.98G [00:15<00:06, 474MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 7.16G/9.98G [00:16<00:05, 472MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 7.21G/9.98G [00:16<00:05, 471MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 7.27G/9.98G [00:16<00:05, 463MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 7.32G/9.98G [00:16<00:05, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 7.37G/9.98G [00:16<00:05, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 7.42G/9.98G [00:16<00:05, 468MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 7.48G/9.98G [00:16<00:05, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 7.53G/9.98G [00:16<00:05, 471MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 7.58G/9.98G [00:16<00:05, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 7.63G/9.98G [00:17<00:05, 465MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 7.69G/9.98G [00:17<00:04, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7.74G/9.98G [00:17<00:04, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7.79G/9.98G [00:17<00:04, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7.84G/9.98G [00:17<00:04, 467MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 7.90G/9.98G [00:17<00:04, 467MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 7.95G/9.98G [00:17<00:04, 468MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8.00G/9.98G [00:17<00:04, 468MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8.05G/9.98G [00:17<00:04, 451MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8.11G/9.98G [00:18<00:04, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 8.16G/9.98G [00:18<00:03, 462MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 8.21G/9.98G [00:18<00:03, 464MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 8.26G/9.98G [00:18<00:03, 464MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 8.32G/9.98G [00:18<00:03, 463MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 8.37G/9.98G [00:18<00:03, 464MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 8.42G/9.98G [00:18<00:03, 465MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 8.47G/9.98G [00:18<00:03, 465MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 8.52G/9.98G [00:18<00:03, 463MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 8.58G/9.98G [00:19<00:03, 464MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 8.63G/9.98G [00:19<00:02, 467MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 8.68G/9.98G [00:19<00:02, 466MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 8.73G/9.98G [00:19<00:02, 467MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 8.79G/9.98G [00:19<00:02, 464MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 8.84G/9.98G [00:19<00:02, 465MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8.89G/9.98G [00:19<00:02, 449MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8.94G/9.98G [00:19<00:02, 455MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9.00G/9.98G [00:19<00:02, 458MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9.05G/9.98G [00:20<00:02, 459MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9.10G/9.98G [00:20<00:01, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9.15G/9.98G [00:20<00:01, 461MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 9.21G/9.98G [00:20<00:01, 463MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9.26G/9.98G [00:20<00:01, 465MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 9.31G/9.98G [00:20<00:01, 468MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 9.36G/9.98G [00:20<00:01, 470MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 9.42G/9.98G [00:20<00:01, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 9.47G/9.98G [00:20<00:01, 468MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 9.52G/9.98G [00:21<00:00, 467MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 9.57G/9.98G [00:21<00:00, 439MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 9.63G/9.98G [00:21<00:00, 383MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 9.68G/9.98G [00:21<00:00, 401MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 9.73G/9.98G [00:21<00:00, 419MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 9.78G/9.98G [00:21<00:00, 432MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 9.84G/9.98G [00:21<00:00, 443MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 9.89G/9.98G [00:22<00:00, 435MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 9.94G/9.98G [00:22<00:00, 436MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.98G/9.98G [00:22<00:00, 449MB/s]\n",
      "Downloading shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:22<00:22, 22.40s/it]\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   1%|          | 21.0M/3.50G [00:00<00:29, 118MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   1%|‚ñè         | 52.4M/3.50G [00:00<00:18, 191MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   3%|‚ñé         | 94.4M/3.50G [00:00<00:13, 260MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   4%|‚ñç         | 147M/3.50G [00:00<00:16, 206MB/s] #033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   5%|‚ñå         | 189M/3.50G [00:00<00:13, 252MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   7%|‚ñã         | 241M/3.50G [00:00<00:10, 308MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   8%|‚ñä         | 283M/3.50G [00:01<00:11, 284MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:   9%|‚ñâ         | 325M/3.50G [00:01<00:10, 314MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  11%|‚ñà         | 377M/3.50G [00:01<00:08, 355MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  12%|‚ñà‚ñè        | 430M/3.50G [00:01<00:07, 385MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  13%|‚ñà‚ñé        | 472M/3.50G [00:01<00:08, 373MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  15%|‚ñà‚ñç        | 524M/3.50G [00:01<00:07, 398MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  16%|‚ñà‚ñã        | 577M/3.50G [00:01<00:06, 418MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  18%|‚ñà‚ñä        | 629M/3.50G [00:01<00:06, 434MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  19%|‚ñà‚ñâ        | 682M/3.50G [00:02<00:06, 444MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  21%|‚ñà‚ñà        | 734M/3.50G [00:02<00:06, 453MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  22%|‚ñà‚ñà‚ñè       | 786M/3.50G [00:02<00:05, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  24%|‚ñà‚ñà‚ñç       | 839M/3.50G [00:02<00:05, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  25%|‚ñà‚ñà‚ñå       | 891M/3.50G [00:02<00:05, 444MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  27%|‚ñà‚ñà‚ñã       | 944M/3.50G [00:02<00:05, 454MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  28%|‚ñà‚ñà‚ñä       | 996M/3.50G [00:02<00:05, 443MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  30%|‚ñà‚ñà‚ñâ       | 1.05G/3.50G [00:02<00:05, 449MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  31%|‚ñà‚ñà‚ñà‚ñè      | 1.10G/3.50G [00:02<00:05, 458MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  33%|‚ñà‚ñà‚ñà‚ñé      | 1.15G/3.50G [00:03<00:05, 464MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  34%|‚ñà‚ñà‚ñà‚ñç      | 1.21G/3.50G [00:03<00:04, 468MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  36%|‚ñà‚ñà‚ñà‚ñå      | 1.26G/3.50G [00:03<00:05, 405MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  37%|‚ñà‚ñà‚ñà‚ñã      | 1.31G/3.50G [00:03<00:05, 379MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  39%|‚ñà‚ñà‚ñà‚ñä      | 1.35G/3.50G [00:03<00:05, 379MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  40%|‚ñà‚ñà‚ñà‚ñâ      | 1.39G/3.50G [00:03<00:05, 363MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  41%|‚ñà‚ñà‚ñà‚ñà      | 1.44G/3.50G [00:03<00:05, 377MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1.49G/3.50G [00:03<00:04, 404MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1.53G/3.50G [00:04<00:04, 402MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1.58G/3.50G [00:04<00:04, 417MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1.64G/3.50G [00:04<00:04, 435MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1.69G/3.50G [00:04<00:04, 446MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1.74G/3.50G [00:04<00:03, 456MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1.79G/3.50G [00:04<00:03, 442MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1.85G/3.50G [00:04<00:03, 451MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1.90G/3.50G [00:04<00:03, 451MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1.95G/3.50G [00:04<00:03, 444MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 2.00G/3.50G [00:05<00:03, 453MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 2.06G/3.50G [00:05<00:03, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 2.11G/3.50G [00:05<00:03, 434MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 2.16G/3.50G [00:05<00:03, 363MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 2.20G/3.50G [00:05<00:03, 327MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2.24G/3.50G [00:05<00:03, 347MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 2.30G/3.50G [00:05<00:03, 380MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2.35G/3.50G [00:05<00:02, 406MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 2.40G/3.50G [00:06<00:02, 427MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2.45G/3.50G [00:06<00:02, 428MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 2.51G/3.50G [00:06<00:02, 441MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2.56G/3.50G [00:06<00:02, 451MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 2.61G/3.50G [00:06<00:01, 457MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 2.66G/3.50G [00:06<00:01, 464MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2.72G/3.50G [00:06<00:01, 468MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 2.77G/3.50G [00:06<00:01, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 2.82G/3.50G [00:06<00:01, 470MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2.87G/3.50G [00:07<00:01, 448MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2.93G/3.50G [00:07<00:01, 455MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2.98G/3.50G [00:07<00:01, 461MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 3.03G/3.50G [00:07<00:01, 464MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 3.08G/3.50G [00:07<00:00, 469MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 3.14G/3.50G [00:07<00:00, 472MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 3.19G/3.50G [00:07<00:00, 474MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 3.24G/3.50G [00:07<00:00, 460MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 3.29G/3.50G [00:08<00:00, 451MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 3.34G/3.50G [00:08<00:00, 459MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3.40G/3.50G [00:08<00:00, 462MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 3.45G/3.50G [00:08<00:00, 467MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.50G/3.50G [00:08<00:00, 472MB/s]#033[A\n",
      "Downloading (‚Ä¶)of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.50G/3.50G [00:08<00:00, 413MB/s]\n",
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:30<00:00, 14.22s/it]\n",
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:30<00:00, 15.45s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.37s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.14it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.05it/s]\n",
      "Downloading (‚Ä¶)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]\n",
      "Downloading (‚Ä¶)neration_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 188/188 [00:00<00:00, 1.30MB/s]\n",
      "Found 7 modules to quantize: ['up_proj', 'down_proj', 'o_proj', 'v_proj', 'q_proj', 'k_proj', 'gate_proj']\n",
      "trainable params: 159,907,840 || all params: 3,660,320,768 || trainable%: 4.368683788535114\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "0%|          | 0/5 [00:00<?, ?it/s]\n",
      "20%|‚ñà‚ñà        | 1/5 [00:08<00:32,  8.11s/it]\n",
      "40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:16<00:24,  8.02s/it]\n",
      "60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:24<00:15,  7.99s/it]\n",
      "80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:31<00:07,  7.98s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:39<00:00,  7.97s/it]\n",
      "{'train_runtime': 39.9406, 'train_samples_per_second': 0.25, 'train_steps_per_second': 0.125, 'train_loss': 1.4927520751953125, 'epoch': 1.0}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:39<00:00,  7.97s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:39<00:00,  7.99s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  7.02it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  5.22it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  5.42it/s]\n",
      "\n",
      "2023-08-21 14:21:17 Uploading - Uploading generated training modelDownloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]\n",
      "Downloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 776/776 [00:00<00:00, 6.81MB/s]\n",
      "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\n",
      "Downloading tokenizer.model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500k/500k [00:00<00:00, 12.6MB/s]\n",
      "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\n",
      "Downloading (‚Ä¶)/main/tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.84M/1.84M [00:00<00:00, 29.8MB/s]\n",
      "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]\n",
      "Downloading (‚Ä¶)cial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 414/414 [00:00<00:00, 5.55MB/s]\n",
      "2023-08-21 14:21:10,850 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-08-21 14:21:10,851 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-08-21 14:21:10,851 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2023-08-21 14:37:30 Completed - Training job completed\n",
      "Training seconds: 1640\n",
      "Billable seconds: 1640\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example for LLaMA 13B, the SageMaker training job took `31728 seconds`, which is about `8.8 hours`. The ml.g5.4xlarge instance we used costs `$2.03 per hour` for on-demand usage. As a result, the total cost for training our fine-tuned LLaMa 2 model was only ~`$18`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps \n",
    "\n",
    "You can deploy your fine-tuned LLaMA model to a SageMaker endpoint and use it for inference. Check out the [Deploy Falcon 7B & 40B on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-llm) and [Securely deploy LLMs inside VPCs with Hugging Face and Amazon SageMaker](https://www.philschmid.de/sagemaker-llm-vpc) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy \"Just Trained\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-qlora-dolly-example-2023-08-21-14-39-23-226\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-qlora-dolly-example-2023-08-21-14-39-23-226\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-qlora-dolly-example-2023-08-21-14-39-23-226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.4xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Model from S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.huggingface.model.HuggingFaceModel object at 0x15673b430>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "s3_model_uri = \"s3://sagemaker-ms-thesis-llm/models/huggingface-qlora-notebook-optDir-2023-08-20-20-01-03-748/output/model.tar.gz\"\n",
    "image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04-v1.0\"\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),\n",
    "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\",# Comment in to quantize\n",
    "# \n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    model_data=s3_model_uri,\n",
    "    env=config\n",
    ")\n",
    "\n",
    "print(llm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.tokens:Loading cached SSO token for slu-sso\n",
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2023-08-21-16-34-04-652\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2023-08-21-16-34-05-592\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2023-08-21-16-34-05-592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Input Data for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple String Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "   \"inputs\": \"What is the Capital of California.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\end{code}\n",
      "\n",
      "I want to get the value of the input as \"S\n"
     ]
    }
   ],
   "source": [
    "chat = llm.predict({\"inputs\":json.dumps(data)})\n",
    "\n",
    "print(chat[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<h1>What is the Capital of California.</h1>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>The capital of California is Sacramento.</p>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>The capital of California is Sacramento.</p>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>The capital of California is Sacramento.</p>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>The capital of California is Sacramento.</p>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>The capital of California is Sacramento.</p>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>The capital of California is Sacramento.</p>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>The capital of California is Sacramento.</p>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>The capital of California is Sacramento.</p>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>The capital of California is Sacramento.</p>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>The capital of California is Sacramento.</p>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>The capital of California is Sacramento.</p>\n",
      "  </div>\n",
      "</div>\n",
      "\n",
      "<div class=\"question\">\n",
      "  <div class=\"question-body\">\n",
      "    <p>\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "  \"inputs\":  json.dumps(data),\n",
    "  \"parameters\": {\n",
    "    # \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    # \"stop\": [\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '\\n\\\\end{code}\\n\\nI want to get the value of the input as \"S'}\n"
     ]
    }
   ],
   "source": [
    "print(chat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatted String Input\n",
    "Format matching how Training Data was Formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt: <s>[INST] <<SYS>>\n",
      "You are a friendly and knowledgeable vacation planning assistant named Clara. Your goal is to answer their questions to help them plan their perfect vacation. \n",
      "<</SYS>>\n",
      "\n",
      "Capital of CA? [/INST]\n"
     ]
    }
   ],
   "source": [
    "def build_llama2_prompt(messages):\n",
    "    startPrompt = \"<s>[INST] \"\n",
    "    endPrompt = \" [/INST]\"\n",
    "    conversation = []\n",
    "    for index, message in enumerate(messages):\n",
    "        if message[\"role\"] == \"system\" and index == 0:\n",
    "            conversation.append(f\"<<SYS>>\\n{message['content']}\\n<</SYS>>\\n\\n\")\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            conversation.append(message[\"content\"].strip())\n",
    "        else:\n",
    "            conversation.append(f\" [/INST] {message['content'].strip()}</s><s>[INST] \")\n",
    "\n",
    "    return startPrompt + \"\".join(conversation) + endPrompt\n",
    "\n",
    "messages = [\n",
    "  { \"role\": \"system\",\"content\": \"You are a friendly and knowledgeable vacation planning assistant named Clara. Your goal is to answer their questions to help them plan their perfect vacation. \"}\n",
    "]\n",
    "\n",
    "# define question and add to messages\n",
    "instruction = \"Capital of CA?\"\n",
    "messages.append({\"role\": \"user\", \"content\": instruction})\n",
    "prompt = build_llama2_prompt(messages)\n",
    "\n",
    "print(f\"Formatted Prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INST] Capital of CA? [/INST]\n",
      "\n",
      "[INST] Capital of\n"
     ]
    }
   ],
   "source": [
    "chat = llm.predict({\"inputs\":prompt})\n",
    "\n",
    "print(chat[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '\\n\\n[INST] Capital of CA? [/INST]\\n\\n[INST] Capital of'}\n"
     ]
    }
   ],
   "source": [
    "print(chat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <<SYS>>\n",
      "\n",
      "<</SYS>>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "  \"inputs\":  prompt,\n",
    "  \"parameters\": {\n",
    "    # \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "print(response[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
