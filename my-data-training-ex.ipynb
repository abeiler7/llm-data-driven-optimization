{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets[s3]==2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_PROFILE=dev-admin\n",
      "env: AWS_REGION=us-east-1\n",
      "env: HF_HOME=~/.cache/huggingface\n",
      "env: TOKENIZERS_PARALLELISM=fale\n"
     ]
    }
   ],
   "source": [
    "%env AWS_PROFILE=dev-admin\n",
    "%env AWS_REGION=us-east-1\n",
    "%env HF_HOME=~/.cache/huggingface\n",
    "%env TOKENIZERS_PARALLELISM=fale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.tokens:Loading cached SSO token for slu-sso\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker bucket: sagemaker-ms-thesis-llm\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "from scripts.aws_init import init_sagemaker\n",
    "\n",
    "sagemaker_session_bucket = \"sagemaker-ms-thesis-llm\"\n",
    "# role = \"arn:aws:iam::171706357329:role/service-role/SageMaker-ComputeAdmin\"\n",
    "role = \"arn:aws:iam::171706357329:role/service-role/AmazonSageMakerServiceCatalogProductsExecutionRole\"\n",
    "\n",
    "sess = init_sagemaker(sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is: \n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code. The model will be temporally offloaded to disk, if it is too large to fit into memory.\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n",
    "### Harwarde requirements\n",
    "\n",
    "We also ran several experiments to determine, which instance type can be used for the different model sizes. The following table shows the results of our experiments. The table shows the instance type, model size, context length, and max batch size. \n",
    "\n",
    "| Model        | Instance Type     | Max Batch Size | Context Length |\n",
    "|--------------|-------------------|----------------|----------------|\n",
    "| [LLama 7B]() | `(ml.)g5.4xlarge` | `3`            | `2048`         |\n",
    "| [LLama 13B]() | `(ml.)g5.4xlarge` | `2`            | `2048`         |\n",
    "| [LLama 70B]() | `(ml.)p4d.24xlarge` | `1++` (need to test more configs)            | `2048`         |\n",
    "\n",
    "\n",
    "> You can also use `g5.2xlarge` instead of the `g5.4xlarge` instance type, but then it is not possible to use `merge_weights` parameter, since to merge the LoRA weights into the model weights, the model needs to fit into memory. But you could save the adapter weights and merge them using [merge_adapter_weights.py](./scripts/merge_adapter_weights.py) after training.\n",
    "\n",
    "_Note: We plan to extend this list in the future. feel free to contribute your setup!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'goatOrig-QLORA-test'\n",
    "model_output_path = f's3://{sagemaker_session_bucket}/models'\n",
    "\n",
    "use_spot_instances = True # wether to use spot instances or not\n",
    "max_wait = 400000 # max time including spot start + training time\n",
    "max_run = 175000 # max expected training time\n",
    "checkpoint_s3_uri = f's3://{sagemaker_session_bucket}/checkpoints/{job_name}-9.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  # 'train_dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  # 'val_dataset_path': '/opt/ml/input/data/validation',    # path where sagemaker will save validation dataset\n",
    "  'dataset': \"abeiler/GOAT\",\n",
    "  'epochs': 1,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 4,                 # batch size for training\n",
    "  'lr': 1e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "  'lora_r': 64,\n",
    "  'lora_alpha': 16,\n",
    "  'lora_dropout': 0.1,\n",
    "  'output_data_path': '/opt/ml/output',\n",
    "  'push_to_hub': True,                            # Defines if we want to push the model to the hub\n",
    "  'hub_model_id': job_name, # The model id of the model to push to the hub\n",
    "  'hub_strategy': 'every_save',                   # The strategy to use when pushing the model to the hub\n",
    "  'hub_token': HfFolder.get_token()   \n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'phil-examples',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    output_path          = f\"{model_output_path}/\",\n",
    "    code_location        = model_output_path,\n",
    "    # use_spot_instances   = use_spot_instances,# wether to use spot instances or not\n",
    "    # max_wait             = max_wait,          # max time including spot start + training time\n",
    "    # max_run              = max_run,           # max expected training time\n",
    "    # checkpoint_s3_uri    = checkpoint_s3_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-hf\n",
      "goatOrig-QLORA-test\n"
     ]
    }
   ],
   "source": [
    "print(model_id)\n",
    "print(job_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: goatOrig-QLORA-test-2023-09-18-01-53-22-542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-18 01:53:23 Starting - Starting the training job...\n",
      "2023-09-18 01:53:39 Starting - Preparing the instances for training......\n",
      "2023-09-18 01:54:37 Downloading - Downloading input data...\n",
      "2023-09-18 01:55:02 Training - Downloading the training image...........................\n",
      "2023-09-18 02:00:03 Training - Training image download completed. Training in progress........bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-09-18 02:01:01,616 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-09-18 02:01:01,629 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-18 02:01:01,638 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-09-18 02:01:01,639 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-09-18 02:01:02,979 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.10 -m pip install -r requirements.txt\n",
      "Collecting transformers>=4.31.0 (from -r requirements.txt (line 1))\n",
      "Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 60.3 MB/s eta 0:00:00\n",
      "Collecting peft==0.4.0 (from -r requirements.txt (line 2))\n",
      "Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 14.5 MB/s eta 0:00:00\n",
      "Collecting accelerate==0.21.0 (from -r requirements.txt (line 3))\n",
      "Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 35.9 MB/s eta 0:00:00\n",
      "Collecting bitsandbytes==0.40.2 (from -r requirements.txt (line 4))\n",
      "Downloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.5/92.5 MB 19.2 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from -r requirements.txt (line 5))\n",
      "Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 82.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tokenizers>=0.13.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.13.3)\n",
      "Collecting sagemaker-training==4.7.0 (from -r requirements.txt (line 7))\n",
      "Downloading sagemaker_training-4.7.0.tar.gz (59 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.1/59.1 kB 15.2 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting auto-gptq==0.4.2 (from -r requirements.txt (line 8))\n",
      "Downloading auto_gptq-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 73.8 MB/s eta 0:00:00\n",
      "Collecting optimum (from -r requirements.txt (line 9))\n",
      "Downloading optimum-1.13.1-py3-none-any.whl (396 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 396.1/396.1 kB 39.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 2)) (2.0.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from sagemaker-training==4.7.0->-r requirements.txt (line 7)) (1.26.132)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from sagemaker-training==4.7.0->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (from sagemaker-training==4.7.0->-r requirements.txt (line 7)) (23.1.2)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker-training==4.7.0->-r requirements.txt (line 7)) (1.3.4)\n",
      "Requirement already satisfied: gevent in /opt/conda/lib/python3.10/site-packages (from sagemaker-training==4.7.0->-r requirements.txt (line 7)) (22.10.2)\n",
      "Requirement already satisfied: inotify_simple==1.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker-training==4.7.0->-r requirements.txt (line 7)) (1.2.1)\n",
      "Requirement already satisfied: werkzeug>=0.15.5 in /opt/conda/lib/python3.10/site-packages (from sagemaker-training==4.7.0->-r requirements.txt (line 7)) (2.3.4)\n",
      "Requirement already satisfied: paramiko>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from sagemaker-training==4.7.0->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: protobuf<=3.20.3,>=3.9.2 in /opt/conda/lib/python3.10/site-packages (from sagemaker-training==4.7.0->-r requirements.txt (line 7)) (3.20.2)\n",
      "Requirement already satisfied: scipy>=1.2.2 in /opt/conda/lib/python3.10/site-packages (from sagemaker-training==4.7.0->-r requirements.txt (line 7)) (1.10.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from auto-gptq==0.4.2->-r requirements.txt (line 8)) (2.12.0)\n",
      "Collecting rouge (from auto-gptq==0.4.2->-r requirements.txt (line 8))\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->-r requirements.txt (line 1)) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers>=4.31.0->-r requirements.txt (line 1))\n",
      "Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.8/294.8 kB 44.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->-r requirements.txt (line 1)) (2023.5.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->-r requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->-r requirements.txt (line 1)) (4.65.0)\n",
      "Collecting coloredlogs (from optimum->-r requirements.txt (line 9))\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 13.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum->-r requirements.txt (line 9)) (1.11.1)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from optimum->-r requirements.txt (line 9)) (4.28.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.31.0->-r requirements.txt (line 1)) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.31.0->-r requirements.txt (line 1)) (4.5.0)\n",
      "Requirement already satisfied: bcrypt>=3.2 in /opt/conda/lib/python3.10/site-packages (from paramiko>=2.4.2->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (4.0.1)\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.10/site-packages (from paramiko>=2.4.2->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (40.0.1)\n",
      "Requirement already satisfied: pynacl>=1.5 in /opt/conda/lib/python3.10/site-packages (from paramiko>=2.4.2->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 2)) (3.1.2)\n",
      "INFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->-r requirements.txt (line 1)) (0.1.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=0.15.5->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.132 in /opt/conda/lib/python3.10/site-packages (from boto3->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (1.29.132)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (0.6.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum->-r requirements.txt (line 9))\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 20.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->-r requirements.txt (line 1)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->-r requirements.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: zope.event in /opt/conda/lib/python3.10/site-packages (from gevent->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (4.6)\n",
      "Requirement already satisfied: zope.interface in /opt/conda/lib/python3.10/site-packages (from gevent->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (6.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from gevent->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (65.6.3)\n",
      "Requirement already satisfied: greenlet>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from gevent->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (2.0.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.132->boto3->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.3->paramiko>=2.4.2->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (1.15.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq==0.4.2->-r requirements.txt (line 8)) (2023.3)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4.2->sagemaker-training==4.7.0->-r requirements.txt (line 7)) (2.21)\n",
      "Building wheels for collected packages: sagemaker-training\n",
      "Building wheel for sagemaker-training (setup.py): started\n",
      "Building wheel for sagemaker-training (setup.py): finished with status 'done'\n",
      "Created wheel for sagemaker-training: filename=sagemaker_training-4.7.0-cp310-cp310-linux_x86_64.whl size=78509 sha256=55f6be996b6bc95dce846a9b2bc9a117e72c09135faf58eb3ee183d56f307833\n",
      "Stored in directory: /root/.cache/pip/wheels/3b/48/2b/166c62ba14389a5d7d3fb956220c9e93aeb14bd21dd61a738c\n",
      "Successfully built sagemaker-training\n",
      "Installing collected packages: safetensors, bitsandbytes, rouge, humanfriendly, huggingface-hub, coloredlogs, transformers, accelerate, peft, sagemaker-training, optimum, auto-gptq\n",
      "Attempting uninstall: huggingface-hub\n",
      "Found existing installation: huggingface-hub 0.14.1\n",
      "Uninstalling huggingface-hub-0.14.1:\n",
      "Successfully uninstalled huggingface-hub-0.14.1\n",
      "Attempting uninstall: transformers\n",
      "Found existing installation: transformers 4.28.1\n",
      "Uninstalling transformers-4.28.1:\n",
      "Successfully uninstalled transformers-4.28.1\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.19.0\n",
      "Uninstalling accelerate-0.19.0:\n",
      "Successfully uninstalled accelerate-0.19.0\n",
      "Attempting uninstall: sagemaker-training\n",
      "Found existing installation: sagemaker-training 4.5.0\n",
      "Uninstalling sagemaker-training-4.5.0:\n",
      "Successfully uninstalled sagemaker-training-4.5.0\n",
      "Successfully installed accelerate-0.21.0 auto-gptq-0.4.2 bitsandbytes-0.40.2 coloredlogs-15.0.1 huggingface-hub-0.17.1 humanfriendly-10.0 optimum-1.13.1 peft-0.4.0 rouge-1.0.1 safetensors-0.3.3 sagemaker-training-4.7.0 transformers-4.33.2\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2023-09-18 02:01:17,275 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-09-18 02:01:17,275 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-09-18 02:01:17,290 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-18 02:01:17,313 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-18 02:01:17,335 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-09-18 02:01:17,345 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset\": \"abeiler/GOAT\",\n",
      "        \"epochs\": 1,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"hub_model_id\": \"goatOrig-QLORA-test\",\n",
      "        \"hub_strategy\": \"every_save\",\n",
      "        \"hub_token\": \"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\",\n",
      "        \"lora_alpha\": 16,\n",
      "        \"lora_dropout\": 0.1,\n",
      "        \"lora_r\": 64,\n",
      "        \"lr\": 0.0001,\n",
      "        \"merge_weights\": true,\n",
      "        \"model_id\": \"meta-llama/Llama-2-7b-hf\",\n",
      "        \"output_data_path\": \"/opt/ml/output\",\n",
      "        \"per_device_train_batch_size\": 4,\n",
      "        \"push_to_hub\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"goatOrig-QLORA-test-2023-09-18-01-53-22-542\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ms-thesis-llm/models/goatOrig-QLORA-test-2023-09-18-01-53-22-542/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"dataset\":\"abeiler/GOAT\",\"epochs\":1,\"hf_token\":\"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\",\"hub_model_id\":\"goatOrig-QLORA-test\",\"hub_strategy\":\"every_save\",\"hub_token\":\"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\",\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":64,\"lr\":0.0001,\"merge_weights\":true,\"model_id\":\"meta-llama/Llama-2-7b-hf\",\"output_data_path\":\"/opt/ml/output\",\"per_device_train_batch_size\":4,\"push_to_hub\":true}\n",
      "SM_USER_ENTRY_POINT=run_clm.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"training\",\"validation\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run_clm\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=16\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-ms-thesis-llm/models/goatOrig-QLORA-test-2023-09-18-01-53-22-542/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset\":\"abeiler/GOAT\",\"epochs\":1,\"hf_token\":\"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\",\"hub_model_id\":\"goatOrig-QLORA-test\",\"hub_strategy\":\"every_save\",\"hub_token\":\"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\",\"lora_alpha\":16,\"lora_dropout\":0.1,\"lora_r\":64,\"lr\":0.0001,\"merge_weights\":true,\"model_id\":\"meta-llama/Llama-2-7b-hf\",\"output_data_path\":\"/opt/ml/output\",\"per_device_train_batch_size\":4,\"push_to_hub\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"goatOrig-QLORA-test-2023-09-18-01-53-22-542\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ms-thesis-llm/models/goatOrig-QLORA-test-2023-09-18-01-53-22-542/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\n",
      "SM_USER_ARGS=[\"--dataset\",\"abeiler/GOAT\",\"--epochs\",\"1\",\"--hf_token\",\"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\",\"--hub_model_id\",\"goatOrig-QLORA-test\",\"--hub_strategy\",\"every_save\",\"--hub_token\",\"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\",\"--lora_alpha\",\"16\",\"--lora_dropout\",\"0.1\",\"--lora_r\",\"64\",\"--lr\",\"0.0001\",\"--merge_weights\",\"True\",\"--model_id\",\"meta-llama/Llama-2-7b-hf\",\"--output_data_path\",\"/opt/ml/output\",\"--per_device_train_batch_size\",\"4\",\"--push_to_hub\",\"True\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "SM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\n",
      "SM_HP_DATASET=abeiler/GOAT\n",
      "SM_HP_EPOCHS=1\n",
      "SM_HP_HF_TOKEN=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\n",
      "SM_HP_HUB_MODEL_ID=goatOrig-QLORA-test\n",
      "SM_HP_HUB_STRATEGY=every_save\n",
      "SM_HP_HUB_TOKEN=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\n",
      "SM_HP_LORA_ALPHA=16\n",
      "SM_HP_LORA_DROPOUT=0.1\n",
      "SM_HP_LORA_R=64\n",
      "SM_HP_LR=0.0001\n",
      "SM_HP_MERGE_WEIGHTS=true\n",
      "SM_HP_MODEL_ID=meta-llama/Llama-2-7b-hf\n",
      "SM_HP_OUTPUT_DATA_PATH=/opt/ml/output\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\n",
      "SM_HP_PUSH_TO_HUB=true\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.10 run_clm.py --dataset abeiler/GOAT --epochs 1 --hf_token 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 --hub_model_id goatOrig-QLORA-test --hub_strategy every_save --hub_token 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 --lora_alpha 16 --lora_dropout 0.1 --lora_r 64 --lr 0.0001 --merge_weights True --model_id meta-llama/Llama-2-7b-hf --output_data_path /opt/ml/output --per_device_train_batch_size 4 --push_to_hub True\n",
      "2023-09-18 02:01:17,372 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Logging into the Hugging Face Hub with token hf_weneUFv...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Downloading and preparing dataset json/abeiler--GOAT to /root/.cache/huggingface/datasets/abeiler___json/abeiler--GOAT-ccc9617b10259a82/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n",
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/22.1M [00:00<?, ?B/s]#033[A\n",
      "Downloading data:  49%|████▉     | 10.9M/22.1M [00:00<00:00, 109MB/s]#033[A\n",
      "Downloading data: 100%|██████████| 22.1M/22.1M [00:00<00:00, 113MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1825.20it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 100000 examples [00:00, 314027.88 examples/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/abeiler___json/abeiler--GOAT-ccc9617b10259a82/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 7.84MB/s]\n",
      "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 147MB/s]\n",
      "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 49.7MB/s]\n",
      "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 5.62MB/s]\n",
      "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]\n",
      "Map:   1%|          | 578/100000 [00:00<00:17, 5727.00 examples/s]\n",
      "Map:   1%|▏         | 1358/100000 [00:00<00:18, 5362.68 examples/s]\n",
      "Map:   2%|▏         | 1952/100000 [00:00<00:17, 5584.17 examples/s]\n",
      "Map:   3%|▎         | 2751/100000 [00:00<00:17, 5458.02 examples/s]\n",
      "Map:   4%|▎         | 3589/100000 [00:00<00:17, 5420.34 examples/s]\n",
      "Map:   4%|▍         | 4349/100000 [00:00<00:18, 5291.28 examples/s]\n",
      "Map:   5%|▍         | 4929/100000 [00:00<00:17, 5417.66 examples/s]\n",
      "Map:   6%|▌         | 5696/100000 [00:01<00:17, 5306.02 examples/s]\n",
      "Map:   6%|▋         | 6460/100000 [00:01<00:17, 5232.58 examples/s]\n",
      "Map:   7%|▋         | 7000/100000 [00:01<00:18, 5161.20 examples/s]\n",
      "Map:   8%|▊         | 7576/100000 [00:01<00:17, 5310.55 examples/s]\n",
      "Map:   8%|▊         | 8347/100000 [00:01<00:17, 5247.83 examples/s]\n",
      "Map:   9%|▉         | 8917/100000 [00:01<00:16, 5359.56 examples/s]\n",
      "Map:  10%|▉         | 9692/100000 [00:01<00:17, 5288.53 examples/s]\n",
      "Map:  10%|█         | 10471/100000 [00:01<00:17, 5252.93 examples/s]\n",
      "Map:  11%|█         | 11000/100000 [00:02<00:17, 5161.96 examples/s]\n",
      "Map:  12%|█▏        | 11565/100000 [00:02<00:16, 5282.79 examples/s]\n",
      "Map:  12%|█▏        | 12342/100000 [00:02<00:16, 5242.35 examples/s]\n",
      "Map:  13%|█▎        | 12915/100000 [00:02<00:16, 5361.75 examples/s]\n",
      "Map:  14%|█▎        | 13692/100000 [00:02<00:16, 5295.98 examples/s]\n",
      "Map:  14%|█▍        | 14468/100000 [00:02<00:16, 5254.34 examples/s]\n",
      "Map:  15%|█▌        | 15000/100000 [00:02<00:16, 5156.13 examples/s]\n",
      "Map:  16%|█▌        | 15563/100000 [00:02<00:16, 5274.04 examples/s]\n",
      "Map:  16%|█▋        | 16322/100000 [00:03<00:16, 5196.64 examples/s]\n",
      "Map:  17%|█▋        | 16888/100000 [00:03<00:15, 5311.79 examples/s]\n",
      "Map:  18%|█▊        | 17657/100000 [00:03<00:15, 5245.21 examples/s]\n",
      "Map:  18%|█▊        | 18427/100000 [00:03<00:15, 5203.97 examples/s]\n",
      "Map:  19%|█▉        | 19000/100000 [00:03<00:15, 5135.78 examples/s]\n",
      "Map:  20%|█▉        | 19572/100000 [00:03<00:15, 5278.91 examples/s]\n",
      "Map:  20%|██        | 20347/100000 [00:03<00:15, 5238.79 examples/s]\n",
      "Map:  21%|██        | 20930/100000 [00:03<00:14, 5381.80 examples/s]\n",
      "Map:  22%|██▏       | 21709/100000 [00:04<00:14, 5312.36 examples/s]\n",
      "Map:  22%|██▏       | 22480/100000 [00:04<00:14, 5253.36 examples/s]\n",
      "Map:  23%|██▎       | 23289/100000 [00:04<00:14, 5262.12 examples/s]\n",
      "Map:  24%|██▍       | 23866/100000 [00:04<00:14, 5377.84 examples/s]\n",
      "Map:  25%|██▍       | 24628/100000 [00:04<00:14, 5276.11 examples/s]\n",
      "Map:  25%|██▌       | 25391/100000 [00:04<00:14, 5211.63 examples/s]\n",
      "Map:  26%|██▌       | 25966/100000 [00:04<00:13, 5336.34 examples/s]\n",
      "Map:  27%|██▋       | 26745/100000 [00:05<00:13, 5286.65 examples/s]\n",
      "Map:  27%|██▋       | 27286/100000 [00:05<00:13, 5195.91 examples/s]\n",
      "Map:  28%|██▊       | 27867/100000 [00:05<00:13, 5348.32 examples/s]\n",
      "Map:  29%|██▊       | 28649/100000 [00:05<00:13, 5297.81 examples/s]\n",
      "Map:  29%|██▉       | 29415/100000 [00:05<00:13, 5230.17 examples/s]\n",
      "Map:  30%|██▉       | 29999/100000 [00:05<00:13, 5374.85 examples/s]\n",
      "Map:  31%|███       | 30769/100000 [00:05<00:13, 5289.07 examples/s]\n",
      "Map:  32%|███▏      | 31593/100000 [00:05<00:12, 5280.50 examples/s]\n",
      "Map:  32%|███▏      | 32376/100000 [00:06<00:12, 5258.52 examples/s]\n",
      "Map:  33%|███▎      | 32951/100000 [00:06<00:12, 5371.72 examples/s]\n",
      "Map:  34%|███▎      | 33728/100000 [00:06<00:12, 5306.04 examples/s]\n",
      "Map:  34%|███▍      | 34289/100000 [00:06<00:12, 5224.09 examples/s]\n",
      "Map:  35%|███▍      | 34870/100000 [00:06<00:12, 5369.82 examples/s]\n",
      "Map:  36%|███▌      | 35663/100000 [00:06<00:12, 5336.91 examples/s]\n",
      "Map:  36%|███▋      | 36441/100000 [00:06<00:12, 5283.10 examples/s]\n",
      "Map:  37%|███▋      | 37000/100000 [00:07<00:12, 5226.15 examples/s]\n",
      "Map:  38%|███▊      | 37562/100000 [00:07<00:11, 5324.01 examples/s]\n",
      "Map:  38%|███▊      | 38335/100000 [00:07<00:11, 5260.33 examples/s]\n",
      "Map:  39%|███▉      | 38908/100000 [00:07<00:11, 5374.07 examples/s]\n",
      "Map:  40%|███▉      | 39691/100000 [00:07<00:11, 5317.40 examples/s]\n",
      "Map:  40%|████      | 40463/100000 [00:07<00:11, 5255.49 examples/s]\n",
      "Map:  41%|████      | 41000/100000 [00:07<00:11, 5160.01 examples/s]\n",
      "Map:  42%|████▏     | 41566/100000 [00:07<00:11, 5282.81 examples/s]\n",
      "Map:  42%|████▏     | 42355/100000 [00:08<00:10, 5271.35 examples/s]\n",
      "Map:  43%|████▎     | 42941/100000 [00:08<00:10, 5417.45 examples/s]\n",
      "Map:  44%|████▎     | 43715/100000 [00:08<00:10, 5325.70 examples/s]\n",
      "Map:  44%|████▍     | 44287/100000 [00:08<00:10, 5261.72 examples/s]\n",
      "Map:  45%|████▍     | 44865/100000 [00:08<00:10, 5393.87 examples/s]\n",
      "Map:  46%|████▌     | 45631/100000 [00:08<00:10, 5287.71 examples/s]\n",
      "Map:  46%|████▋     | 46388/100000 [00:08<00:10, 5204.67 examples/s]\n",
      "Map:  47%|████▋     | 46953/100000 [00:08<00:09, 5310.46 examples/s]\n",
      "Map:  48%|████▊     | 47741/100000 [00:09<00:09, 5289.34 examples/s]\n",
      "Map:  48%|████▊     | 48284/100000 [00:09<00:09, 5211.76 examples/s]\n",
      "Map:  49%|████▉     | 48848/100000 [00:09<00:09, 5319.03 examples/s]\n",
      "Map:  50%|████▉     | 49626/100000 [00:09<00:09, 5266.43 examples/s]\n",
      "Map:  50%|█████     | 50401/100000 [00:09<00:09, 5230.89 examples/s]\n",
      "Map:  51%|█████     | 50964/100000 [00:09<00:09, 5324.58 examples/s]\n",
      "Map:  52%|█████▏    | 51730/100000 [00:09<00:09, 5247.89 examples/s]\n",
      "Map:  52%|█████▏    | 52268/100000 [00:09<00:09, 5118.95 examples/s]\n",
      "Map:  53%|█████▎    | 52847/100000 [00:09<00:08, 5286.07 examples/s]\n",
      "Map:  54%|█████▎    | 53609/100000 [00:10<00:08, 5209.20 examples/s]\n",
      "Map:  54%|█████▍    | 54382/100000 [00:10<00:08, 5186.05 examples/s]\n",
      "Map:  55%|█████▍    | 54953/100000 [00:10<00:08, 5310.51 examples/s]\n",
      "Map:  56%|█████▌    | 55719/100000 [00:10<00:08, 5238.13 examples/s]\n",
      "Map:  56%|█████▋    | 56293/100000 [00:10<00:08, 5186.93 examples/s]\n",
      "Map:  57%|█████▋    | 56852/100000 [00:10<00:08, 5286.88 examples/s]\n",
      "Map:  58%|█████▊    | 57636/100000 [00:10<00:08, 5262.32 examples/s]\n",
      "Map:  58%|█████▊    | 58413/100000 [00:11<00:07, 5233.15 examples/s]\n",
      "Map:  59%|█████▉    | 58985/100000 [00:11<00:07, 5345.31 examples/s]\n",
      "Map:  60%|█████▉    | 59760/100000 [00:11<00:07, 5282.33 examples/s]\n",
      "Map:  61%|██████    | 60584/100000 [00:11<00:07, 5266.82 examples/s]\n",
      "Map:  61%|██████▏   | 61364/100000 [00:11<00:07, 5241.23 examples/s]\n",
      "Map:  62%|██████▏   | 61917/100000 [00:11<00:07, 5306.12 examples/s]\n",
      "Map:  63%|██████▎   | 62679/100000 [00:11<00:07, 5227.90 examples/s]\n",
      "Map:  63%|██████▎   | 63452/100000 [00:12<00:07, 5201.51 examples/s]\n",
      "Map:  64%|██████▍   | 64000/100000 [00:12<00:07, 5129.00 examples/s]\n",
      "Map:  65%|██████▍   | 64568/100000 [00:12<00:06, 5262.56 examples/s]\n",
      "Map:  65%|██████▌   | 65351/100000 [00:12<00:06, 5244.04 examples/s]\n",
      "Map:  66%|██████▌   | 65930/100000 [00:12<00:06, 5375.56 examples/s]\n",
      "Map:  67%|██████▋   | 66695/100000 [00:12<00:06, 5277.47 examples/s]\n",
      "Map:  67%|██████▋   | 67466/100000 [00:12<00:06, 5227.91 examples/s]\n",
      "Map:  68%|██████▊   | 68000/100000 [00:12<00:06, 5150.63 examples/s]\n",
      "Map:  69%|██████▊   | 68581/100000 [00:12<00:05, 5313.58 examples/s]\n",
      "Map:  69%|██████▉   | 69332/100000 [00:13<00:05, 5203.21 examples/s]\n",
      "Map:  70%|██████▉   | 69892/100000 [00:13<00:05, 5301.06 examples/s]\n",
      "Map:  71%|███████   | 70665/100000 [00:13<00:05, 5248.38 examples/s]\n",
      "Map:  71%|███████▏  | 71450/100000 [00:13<00:05, 5237.57 examples/s]\n",
      "Map:  72%|███████▏  | 72000/100000 [00:13<00:05, 5149.94 examples/s]\n",
      "Map:  73%|███████▎  | 72570/100000 [00:13<00:05, 5286.42 examples/s]\n",
      "Map:  73%|███████▎  | 73335/100000 [00:13<00:05, 5218.52 examples/s]\n",
      "Map:  74%|███████▍  | 73910/100000 [00:14<00:04, 5347.34 examples/s]\n",
      "Map:  75%|███████▍  | 74667/100000 [00:14<00:04, 5238.51 examples/s]\n",
      "Map:  75%|███████▌  | 75449/100000 [00:14<00:04, 5228.41 examples/s]\n",
      "Map:  76%|███████▌  | 76000/100000 [00:14<00:04, 5156.29 examples/s]\n",
      "Map:  77%|███████▋  | 76582/100000 [00:14<00:04, 5320.75 examples/s]\n",
      "Map:  77%|███████▋  | 77364/100000 [00:14<00:04, 5282.29 examples/s]\n",
      "Map:  78%|███████▊  | 77934/100000 [00:14<00:04, 5384.83 examples/s]\n",
      "Map:  79%|███████▊  | 78711/100000 [00:14<00:04, 5310.95 examples/s]\n",
      "Map:  79%|███████▉  | 79288/100000 [00:15<00:03, 5236.31 examples/s]\n",
      "Map:  80%|███████▉  | 79859/100000 [00:15<00:03, 5353.89 examples/s]\n",
      "Map:  81%|████████  | 80631/100000 [00:15<00:03, 5279.40 examples/s]\n",
      "Map:  81%|████████▏ | 81409/100000 [00:15<00:03, 5246.68 examples/s]\n",
      "Map:  82%|████████▏ | 81994/100000 [00:15<00:03, 5390.39 examples/s]\n",
      "Map:  83%|████████▎ | 82761/100000 [00:15<00:03, 5292.69 examples/s]\n",
      "Map:  84%|████████▎ | 83576/100000 [00:15<00:03, 5253.34 examples/s]\n",
      "Map:  84%|████████▍ | 84337/100000 [00:15<00:03, 5194.08 examples/s]\n",
      "Map:  85%|████████▍ | 84922/100000 [00:16<00:02, 5343.62 examples/s]\n",
      "Map:  86%|████████▌ | 85697/100000 [00:16<00:02, 5282.41 examples/s]\n",
      "Map:  86%|████████▋ | 86465/100000 [00:16<00:02, 5227.42 examples/s]\n",
      "Map:  87%|████████▋ | 87000/100000 [00:16<00:02, 5196.33 examples/s]\n",
      "Map:  88%|████████▊ | 87577/100000 [00:16<00:02, 5333.66 examples/s]\n",
      "Map:  88%|████████▊ | 88339/100000 [00:16<00:02, 5241.92 examples/s]\n",
      "Map:  89%|████████▉ | 88919/100000 [00:16<00:02, 5377.80 examples/s]\n",
      "Map:  90%|████████▉ | 89708/100000 [00:17<00:01, 5332.75 examples/s]\n",
      "Map:  90%|█████████ | 90494/100000 [00:17<00:01, 5298.87 examples/s]\n",
      "Map:  91%|█████████▏| 91287/100000 [00:17<00:01, 5263.08 examples/s]\n",
      "Map:  92%|█████████▏| 91863/100000 [00:17<00:01, 5376.32 examples/s]\n",
      "Map:  93%|█████████▎| 92648/100000 [00:17<00:01, 5325.34 examples/s]\n",
      "Map:  93%|█████████▎| 93412/100000 [00:17<00:01, 5247.38 examples/s]\n",
      "Map:  94%|█████████▍| 93981/100000 [00:17<00:01, 5347.50 examples/s]\n",
      "Map:  95%|█████████▍| 94770/100000 [00:17<00:00, 5315.71 examples/s]\n",
      "Map:  96%|█████████▌| 95563/100000 [00:18<00:00, 5241.83 examples/s]\n",
      "Map:  96%|█████████▋| 96350/100000 [00:18<00:00, 5239.81 examples/s]\n",
      "Map:  97%|█████████▋| 96931/100000 [00:18<00:00, 5368.73 examples/s]\n",
      "Map:  98%|█████████▊| 97707/100000 [00:18<00:00, 5301.62 examples/s]\n",
      "Map:  98%|█████████▊| 98472/100000 [00:18<00:00, 5234.23 examples/s]\n",
      "Map:  99%|█████████▉| 99000/100000 [00:18<00:00, 5158.11 examples/s]\n",
      "Map: 100%|█████████▉| 99575/100000 [00:18<00:00, 5298.04 examples/s]\n",
      "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]\n",
      "Map:   2%|▏         | 2000/100000 [00:00<00:08, 11100.84 examples/s]\n",
      "Map:   4%|▍         | 4000/100000 [00:00<00:08, 11420.82 examples/s]\n",
      "Map:   6%|▌         | 6000/100000 [00:00<00:08, 11541.38 examples/s]\n",
      "Map:   8%|▊         | 8000/100000 [00:00<00:07, 11640.12 examples/s]\n",
      "Map:  10%|█         | 10000/100000 [00:00<00:07, 11609.94 examples/s]\n",
      "Map:  12%|█▏        | 12000/100000 [00:01<00:07, 11559.58 examples/s]\n",
      "Map:  14%|█▍        | 14000/100000 [00:01<00:07, 11620.55 examples/s]\n",
      "Map:  16%|█▌        | 16000/100000 [00:01<00:07, 11597.79 examples/s]\n",
      "Map:  18%|█▊        | 18000/100000 [00:01<00:07, 11558.34 examples/s]\n",
      "Map:  20%|██        | 20000/100000 [00:01<00:06, 11638.42 examples/s]\n",
      "Map:  22%|██▏       | 22000/100000 [00:01<00:06, 11635.62 examples/s]\n",
      "Map:  24%|██▍       | 24000/100000 [00:02<00:06, 11712.71 examples/s]\n",
      "Map:  26%|██▌       | 26000/100000 [00:02<00:06, 11736.28 examples/s]\n",
      "Map:  28%|██▊       | 28000/100000 [00:02<00:06, 11749.29 examples/s]\n",
      "Map:  30%|███       | 30000/100000 [00:02<00:05, 11712.89 examples/s]\n",
      "Map:  32%|███▏      | 32000/100000 [00:02<00:05, 11759.85 examples/s]\n",
      "Map:  34%|███▍      | 34000/100000 [00:02<00:05, 11723.17 examples/s]\n",
      "Map:  36%|███▌      | 36000/100000 [00:03<00:05, 11898.75 examples/s]\n",
      "Map:  38%|███▊      | 38000/100000 [00:03<00:05, 11791.48 examples/s]\n",
      "Map:  40%|████      | 40000/100000 [00:03<00:05, 11857.35 examples/s]\n",
      "Map:  42%|████▏     | 42000/100000 [00:03<00:04, 11825.55 examples/s]\n",
      "Map:  44%|████▍     | 44000/100000 [00:03<00:04, 11904.23 examples/s]\n",
      "Map:  46%|████▌     | 46000/100000 [00:03<00:04, 11790.36 examples/s]\n",
      "Map:  48%|████▊     | 48000/100000 [00:04<00:04, 11762.75 examples/s]\n",
      "Map:  50%|█████     | 50000/100000 [00:04<00:04, 11715.42 examples/s]\n",
      "Map:  52%|█████▏    | 52000/100000 [00:04<00:04, 11718.48 examples/s]\n",
      "Map:  54%|█████▍    | 54000/100000 [00:04<00:03, 11648.31 examples/s]\n",
      "Map:  56%|█████▌    | 56000/100000 [00:04<00:03, 11649.15 examples/s]\n",
      "Map:  58%|█████▊    | 58000/100000 [00:04<00:03, 11774.70 examples/s]\n",
      "Map:  60%|██████    | 60000/100000 [00:05<00:03, 11822.65 examples/s]\n",
      "Map:  62%|██████▏   | 62000/100000 [00:05<00:03, 11727.08 examples/s]\n",
      "Map:  64%|██████▍   | 64000/100000 [00:05<00:03, 11721.83 examples/s]\n",
      "Map:  66%|██████▌   | 66000/100000 [00:05<00:02, 11765.20 examples/s]\n",
      "Map:  68%|██████▊   | 68000/100000 [00:05<00:02, 11735.70 examples/s]\n",
      "Map:  70%|███████   | 70000/100000 [00:05<00:02, 11590.29 examples/s]\n",
      "Map:  72%|███████▏  | 72000/100000 [00:06<00:02, 11616.31 examples/s]\n",
      "Map:  74%|███████▍  | 74000/100000 [00:06<00:02, 11501.65 examples/s]\n",
      "Map:  76%|███████▌  | 76000/100000 [00:06<00:02, 11497.92 examples/s]\n",
      "Map:  78%|███████▊  | 78000/100000 [00:06<00:01, 11647.35 examples/s]\n",
      "Map:  80%|████████  | 80000/100000 [00:06<00:01, 11577.01 examples/s]\n",
      "Map:  82%|████████▏ | 82000/100000 [00:07<00:01, 11533.42 examples/s]\n",
      "Map:  84%|████████▍ | 84000/100000 [00:07<00:01, 11526.97 examples/s]\n",
      "Map:  86%|████████▌ | 86000/100000 [00:07<00:01, 11435.71 examples/s]\n",
      "Map:  88%|████████▊ | 88000/100000 [00:07<00:01, 11504.05 examples/s]\n",
      "Map:  90%|█████████ | 90000/100000 [00:07<00:00, 11565.86 examples/s]\n",
      "Map:  92%|█████████▏| 92000/100000 [00:07<00:00, 11600.04 examples/s]\n",
      "Map:  94%|█████████▍| 94000/100000 [00:08<00:00, 11586.05 examples/s]\n",
      "Map:  96%|█████████▌| 96000/100000 [00:08<00:00, 11630.36 examples/s]\n",
      "Map:  98%|█████████▊| 98000/100000 [00:08<00:00, 11519.64 examples/s]\n",
      "Map: 100%|██████████| 100000/100000 [00:08<00:00, 11487.73 examples/s]\n",
      "Downloading (…)lve/main/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 609/609 [00:00<00:00, 7.40MB/s]\n",
      "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]\n",
      "Downloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 198MB/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   0%|          | 41.9M/9.98G [00:00<00:25, 393MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   1%|          | 94.4M/9.98G [00:00<00:22, 437MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   1%|▏         | 147M/9.98G [00:00<00:21, 450MB/s] #033[A\n",
      "Downloading (…)of-00002.safetensors:   2%|▏         | 199M/9.98G [00:00<00:21, 459MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   3%|▎         | 252M/9.98G [00:00<00:21, 461MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   3%|▎         | 304M/9.98G [00:00<00:20, 464MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   4%|▎         | 357M/9.98G [00:00<00:20, 467MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   4%|▍         | 409M/9.98G [00:00<00:20, 470MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   5%|▍         | 461M/9.98G [00:00<00:20, 470MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   5%|▌         | 514M/9.98G [00:01<00:20, 470MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   6%|▌         | 566M/9.98G [00:01<00:19, 472MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   6%|▌         | 619M/9.98G [00:01<00:19, 471MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   7%|▋         | 671M/9.98G [00:01<00:19, 469MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   7%|▋         | 724M/9.98G [00:01<00:19, 470MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   8%|▊         | 776M/9.98G [00:01<00:19, 471MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   8%|▊         | 828M/9.98G [00:01<00:19, 471MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   9%|▉         | 881M/9.98G [00:01<00:19, 469MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   9%|▉         | 933M/9.98G [00:02<00:20, 437MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  10%|▉         | 986M/9.98G [00:02<00:21, 424MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  10%|█         | 1.04G/9.98G [00:02<00:20, 435MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  11%|█         | 1.09G/9.98G [00:02<00:19, 445MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  11%|█▏        | 1.14G/9.98G [00:02<00:19, 453MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  12%|█▏        | 1.20G/9.98G [00:02<00:19, 460MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  13%|█▎        | 1.25G/9.98G [00:02<00:18, 464MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  13%|█▎        | 1.30G/9.98G [00:02<00:18, 466MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  14%|█▎        | 1.35G/9.98G [00:02<00:18, 468MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  14%|█▍        | 1.41G/9.98G [00:03<00:24, 356MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  15%|█▍        | 1.46G/9.98G [00:03<00:22, 381MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  15%|█▌        | 1.51G/9.98G [00:03<00:21, 402MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  16%|█▌        | 1.56G/9.98G [00:03<00:20, 418MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  16%|█▌        | 1.61G/9.98G [00:03<00:19, 431MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  17%|█▋        | 1.67G/9.98G [00:03<00:18, 444MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  17%|█▋        | 1.72G/9.98G [00:03<00:18, 451MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  18%|█▊        | 1.77G/9.98G [00:03<00:17, 457MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  18%|█▊        | 1.82G/9.98G [00:04<00:17, 461MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  19%|█▉        | 1.88G/9.98G [00:04<00:18, 445MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  19%|█▉        | 1.93G/9.98G [00:04<00:17, 450MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  20%|█▉        | 1.98G/9.98G [00:04<00:17, 457MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  20%|██        | 2.03G/9.98G [00:04<00:17, 462MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  21%|██        | 2.09G/9.98G [00:04<00:16, 465MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  21%|██▏       | 2.14G/9.98G [00:04<00:16, 468MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  22%|██▏       | 2.19G/9.98G [00:04<00:16, 470MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  22%|██▏       | 2.24G/9.98G [00:04<00:16, 470MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  23%|██▎       | 2.30G/9.98G [00:05<00:16, 470MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  24%|██▎       | 2.35G/9.98G [00:05<00:18, 424MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  24%|██▍       | 2.40G/9.98G [00:05<00:17, 422MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  25%|██▍       | 2.45G/9.98G [00:05<00:18, 412MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  25%|██▌       | 2.50G/9.98G [00:05<00:19, 386MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  25%|██▌       | 2.54G/9.98G [00:05<00:19, 384MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  26%|██▌       | 2.58G/9.98G [00:05<00:19, 375MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  26%|██▋       | 2.62G/9.98G [00:05<00:20, 361MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  27%|██▋       | 2.67G/9.98G [00:06<00:19, 383MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  27%|██▋       | 2.72G/9.98G [00:06<00:20, 362MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  28%|██▊       | 2.77G/9.98G [00:06<00:18, 391MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  28%|██▊       | 2.82G/9.98G [00:06<00:17, 412MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  29%|██▉       | 2.87G/9.98G [00:06<00:16, 419MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  29%|██▉       | 2.93G/9.98G [00:06<00:17, 397MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  30%|██▉       | 2.97G/9.98G [00:06<00:18, 373MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  30%|███       | 3.02G/9.98G [00:06<00:17, 395MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  31%|███       | 3.07G/9.98G [00:07<00:16, 416MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  31%|███▏      | 3.12G/9.98G [00:07<00:15, 429MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  32%|███▏      | 3.18G/9.98G [00:07<00:15, 441MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  32%|███▏      | 3.23G/9.98G [00:07<00:15, 444MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  33%|███▎      | 3.28G/9.98G [00:07<00:15, 445MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  33%|███▎      | 3.33G/9.98G [00:07<00:15, 433MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  34%|███▍      | 3.39G/9.98G [00:07<00:14, 444MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  34%|███▍      | 3.44G/9.98G [00:07<00:14, 452MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  35%|███▍      | 3.49G/9.98G [00:08<00:14, 456MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  36%|███▌      | 3.54G/9.98G [00:08<00:14, 459MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  36%|███▌      | 3.60G/9.98G [00:08<00:15, 423MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  37%|███▋      | 3.65G/9.98G [00:08<00:15, 407MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  37%|███▋      | 3.69G/9.98G [00:08<00:16, 391MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  38%|███▊      | 3.74G/9.98G [00:08<00:15, 404MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  38%|███▊      | 3.80G/9.98G [00:08<00:15, 396MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  38%|███▊      | 3.84G/9.98G [00:08<00:15, 386MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  39%|███▉      | 3.88G/9.98G [00:09<00:16, 365MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  39%|███▉      | 3.92G/9.98G [00:09<00:16, 367MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  40%|███▉      | 3.97G/9.98G [00:09<00:15, 393MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  40%|████      | 4.03G/9.98G [00:09<00:14, 415MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  41%|████      | 4.08G/9.98G [00:09<00:13, 428MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  41%|████▏     | 4.13G/9.98G [00:09<00:14, 397MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  42%|████▏     | 4.18G/9.98G [00:09<00:14, 412MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  42%|████▏     | 4.24G/9.98G [00:09<00:13, 426MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  43%|████▎     | 4.29G/9.98G [00:09<00:13, 428MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  44%|████▎     | 4.34G/9.98G [00:10<00:13, 424MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  44%|████▍     | 4.39G/9.98G [00:10<00:13, 426MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  45%|████▍     | 4.45G/9.98G [00:10<00:12, 437MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  45%|████▌     | 4.50G/9.98G [00:10<00:12, 448MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  46%|████▌     | 4.55G/9.98G [00:10<00:11, 453MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  46%|████▌     | 4.60G/9.98G [00:10<00:11, 459MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  47%|████▋     | 4.66G/9.98G [00:10<00:11, 464MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  47%|████▋     | 4.71G/9.98G [00:10<00:11, 466MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  48%|████▊     | 4.76G/9.98G [00:11<00:12, 402MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  48%|████▊     | 4.81G/9.98G [00:11<00:12, 419MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  49%|████▉     | 4.87G/9.98G [00:11<00:11, 433MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  49%|████▉     | 4.92G/9.98G [00:11<00:11, 443MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  50%|████▉     | 4.97G/9.98G [00:11<00:11, 452MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  50%|█████     | 5.02G/9.98G [00:11<00:10, 455MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  51%|█████     | 5.08G/9.98G [00:11<00:10, 461MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  51%|█████▏    | 5.13G/9.98G [00:11<00:10, 457MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.18G/9.98G [00:11<00:10, 460MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  52%|█████▏    | 5.23G/9.98G [00:12<00:10, 459MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.28G/9.98G [00:12<00:10, 459MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  53%|█████▎    | 5.34G/9.98G [00:12<00:10, 459MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  54%|█████▍    | 5.39G/9.98G [00:12<00:10, 450MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  55%|█████▍    | 5.44G/9.98G [00:12<00:11, 406MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  55%|█████▌    | 5.49G/9.98G [00:12<00:10, 416MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.55G/9.98G [00:12<00:11, 389MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  56%|█████▌    | 5.59G/9.98G [00:13<00:12, 363MB/s]\n",
      "#033[A\n",
      "Downloading (…)of-00002.safetensors:  56%|█████▋    | 5.63G/9.98G [00:13<00:11, 364MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.68G/9.98G [00:13<00:11, 380MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  57%|█████▋    | 5.73G/9.98G [00:13<00:11, 372MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.77G/9.98G [00:13<00:11, 361MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  58%|█████▊    | 5.82G/9.98G [00:13<00:10, 383MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.87G/9.98G [00:13<00:10, 404MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  59%|█████▉    | 5.92G/9.98G [00:13<00:09, 421MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  60%|█████▉    | 5.98G/9.98G [00:13<00:09, 431MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  60%|██████    | 6.03G/9.98G [00:14<00:09, 438MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  61%|██████    | 6.08G/9.98G [00:14<00:08, 447MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  61%|██████▏   | 6.13G/9.98G [00:14<00:08, 449MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  62%|██████▏   | 6.19G/9.98G [00:14<00:08, 453MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.24G/9.98G [00:14<00:08, 456MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  63%|██████▎   | 6.29G/9.98G [00:14<00:08, 460MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  64%|██████▎   | 6.34G/9.98G [00:14<00:07, 462MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  64%|██████▍   | 6.40G/9.98G [00:14<00:07, 461MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  65%|██████▍   | 6.45G/9.98G [00:14<00:07, 462MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  65%|██████▌   | 6.50G/9.98G [00:15<00:07, 463MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.55G/9.98G [00:15<00:07, 463MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  66%|██████▌   | 6.61G/9.98G [00:15<00:07, 447MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.66G/9.98G [00:15<00:08, 397MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  67%|██████▋   | 6.70G/9.98G [00:15<00:08, 386MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.75G/9.98G [00:15<00:08, 403MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  68%|██████▊   | 6.81G/9.98G [00:15<00:07, 418MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  69%|██████▊   | 6.86G/9.98G [00:15<00:07, 420MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  69%|██████▉   | 6.91G/9.98G [00:16<00:07, 387MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  70%|██████▉   | 6.96G/9.98G [00:16<00:07, 404MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  70%|███████   | 7.00G/9.98G [00:16<00:08, 360MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  71%|███████   | 7.05G/9.98G [00:16<00:08, 343MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  71%|███████   | 7.09G/9.98G [00:16<00:10, 283MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  71%|███████▏  | 7.13G/9.98G [00:16<00:09, 311MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  72%|███████▏  | 7.18G/9.98G [00:16<00:07, 350MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.24G/9.98G [00:17<00:07, 380MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  73%|███████▎  | 7.29G/9.98G [00:17<00:06, 400MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  74%|███████▎  | 7.34G/9.98G [00:17<00:06, 419MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  74%|███████▍  | 7.39G/9.98G [00:17<00:06, 413MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  75%|███████▍  | 7.44G/9.98G [00:17<00:05, 425MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  75%|███████▌  | 7.50G/9.98G [00:17<00:05, 440MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.55G/9.98G [00:17<00:05, 451MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  76%|███████▌  | 7.60G/9.98G [00:17<00:05, 448MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.65G/9.98G [00:18<00:05, 438MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  77%|███████▋  | 7.71G/9.98G [00:18<00:05, 424MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.76G/9.98G [00:18<00:05, 425MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  78%|███████▊  | 7.81G/9.98G [00:18<00:04, 439MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.86G/9.98G [00:18<00:04, 447MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  79%|███████▉  | 7.92G/9.98G [00:18<00:04, 449MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  80%|███████▉  | 7.97G/9.98G [00:18<00:04, 450MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  80%|████████  | 8.02G/9.98G [00:18<00:04, 441MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  81%|████████  | 8.07G/9.98G [00:18<00:04, 446MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  81%|████████▏ | 8.13G/9.98G [00:19<00:04, 409MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.17G/9.98G [00:19<00:04, 391MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  82%|████████▏ | 8.22G/9.98G [00:19<00:04, 411MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.26G/9.98G [00:19<00:04, 376MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  83%|████████▎ | 8.32G/9.98G [00:19<00:04, 399MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.37G/9.98G [00:19<00:03, 419MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  84%|████████▍ | 8.42G/9.98G [00:19<00:03, 433MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  85%|████████▍ | 8.47G/9.98G [00:19<00:03, 444MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  85%|████████▌ | 8.52G/9.98G [00:20<00:03, 412MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  86%|████████▌ | 8.58G/9.98G [00:20<00:03, 425MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.63G/9.98G [00:20<00:03, 374MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.68G/9.98G [00:20<00:03, 393MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  87%|████████▋ | 8.72G/9.98G [00:20<00:03, 380MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.78G/9.98G [00:20<00:03, 387MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  88%|████████▊ | 8.83G/9.98G [00:20<00:02, 398MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.87G/9.98G [00:21<00:02, 386MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  89%|████████▉ | 8.92G/9.98G [00:21<00:02, 403MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  90%|████████▉ | 8.98G/9.98G [00:21<00:02, 422MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  90%|█████████ | 9.03G/9.98G [00:21<00:02, 434MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  91%|█████████ | 9.08G/9.98G [00:21<00:02, 443MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.13G/9.98G [00:21<00:01, 450MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  92%|█████████▏| 9.19G/9.98G [00:21<00:01, 456MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.24G/9.98G [00:21<00:01, 418MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  93%|█████████▎| 9.29G/9.98G [00:21<00:01, 431MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█████████▎| 9.34G/9.98G [00:22<00:01, 441MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█████████▍| 9.40G/9.98G [00:22<00:01, 429MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  95%|█████████▍| 9.45G/9.98G [00:22<00:01, 394MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  95%|█████████▌| 9.50G/9.98G [00:22<00:01, 411MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  96%|█████████▌| 9.55G/9.98G [00:22<00:01, 420MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  96%|█████████▋| 9.60G/9.98G [00:22<00:00, 424MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.66G/9.98G [00:22<00:00, 436MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  97%|█████████▋| 9.71G/9.98G [00:22<00:00, 444MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.76G/9.98G [00:23<00:00, 450MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  98%|█████████▊| 9.81G/9.98G [00:23<00:00, 453MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  99%|█████████▉| 9.87G/9.98G [00:23<00:00, 456MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  99%|█████████▉| 9.92G/9.98G [00:23<00:00, 460MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors: 100%|█████████▉| 9.97G/9.98G [00:23<00:00, 464MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:23<00:00, 425MB/s]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:23<00:23, 23.52s/it]\n",
      "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   1%|          | 41.9M/3.50G [00:00<00:08, 418MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   3%|▎         | 94.4M/3.50G [00:00<00:07, 450MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   4%|▍         | 147M/3.50G [00:00<00:07, 454MB/s] #033[A\n",
      "Downloading (…)of-00002.safetensors:   6%|▌         | 199M/3.50G [00:00<00:07, 451MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   7%|▋         | 252M/3.50G [00:00<00:07, 453MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:   9%|▊         | 304M/3.50G [00:00<00:06, 457MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  10%|█         | 357M/3.50G [00:00<00:06, 455MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  12%|█▏        | 409M/3.50G [00:00<00:06, 460MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  13%|█▎        | 461M/3.50G [00:01<00:06, 462MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  15%|█▍        | 514M/3.50G [00:01<00:06, 465MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  16%|█▌        | 566M/3.50G [00:01<00:06, 465MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  18%|█▊        | 619M/3.50G [00:01<00:06, 466MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  19%|█▉        | 671M/3.50G [00:01<00:06, 463MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  21%|██        | 724M/3.50G [00:01<00:05, 464MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  22%|██▏       | 776M/3.50G [00:01<00:05, 465MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  24%|██▎       | 828M/3.50G [00:01<00:05, 462MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  25%|██▌       | 881M/3.50G [00:01<00:05, 463MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  27%|██▋       | 933M/3.50G [00:02<00:06, 404MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  28%|██▊       | 986M/3.50G [00:02<00:06, 411MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  30%|██▉       | 1.04G/3.50G [00:02<00:05, 426MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  31%|███       | 1.09G/3.50G [00:03<00:14, 161MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  32%|███▏      | 1.13G/3.50G [00:03<00:12, 188MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  34%|███▎      | 1.17G/3.50G [00:03<00:11, 209MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  35%|███▌      | 1.23G/3.50G [00:03<00:09, 252MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  36%|███▌      | 1.27G/3.50G [00:03<00:08, 275MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  38%|███▊      | 1.32G/3.50G [00:03<00:06, 314MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  39%|███▉      | 1.37G/3.50G [00:03<00:06, 341MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  40%|████      | 1.42G/3.50G [00:03<00:06, 347MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  42%|████▏     | 1.47G/3.50G [00:04<00:05, 372MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  43%|████▎     | 1.52G/3.50G [00:04<00:04, 398MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  45%|████▍     | 1.57G/3.50G [00:04<00:04, 414MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  46%|████▋     | 1.63G/3.50G [00:04<00:05, 366MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  48%|████▊     | 1.68G/3.50G [00:04<00:04, 389MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  49%|████▉     | 1.73G/3.50G [00:04<00:04, 390MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  51%|█████     | 1.78G/3.50G [00:04<00:04, 406MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  52%|█████▏    | 1.84G/3.50G [00:04<00:03, 422MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  54%|█████▍    | 1.89G/3.50G [00:05<00:03, 436MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  55%|█████▌    | 1.94G/3.50G [00:05<00:03, 444MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  57%|█████▋    | 1.99G/3.50G [00:05<00:03, 451MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  58%|█████▊    | 2.04G/3.50G [00:05<00:03, 457MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  60%|█████▉    | 2.10G/3.50G [00:05<00:03, 460MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  61%|██████▏   | 2.15G/3.50G [00:05<00:02, 462MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  63%|██████▎   | 2.20G/3.50G [00:05<00:02, 464MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  64%|██████▍   | 2.25G/3.50G [00:05<00:02, 464MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  66%|██████▌   | 2.31G/3.50G [00:05<00:02, 464MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  67%|██████▋   | 2.36G/3.50G [00:06<00:02, 466MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  69%|██████▉   | 2.41G/3.50G [00:06<00:02, 464MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  70%|███████   | 2.46G/3.50G [00:06<00:02, 413MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  72%|███████▏  | 2.52G/3.50G [00:06<00:02, 422MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  73%|███████▎  | 2.57G/3.50G [00:06<00:02, 432MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  75%|███████▍  | 2.62G/3.50G [00:06<00:01, 439MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  76%|███████▋  | 2.67G/3.50G [00:06<00:01, 445MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  78%|███████▊  | 2.73G/3.50G [00:06<00:01, 451MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  79%|███████▉  | 2.78G/3.50G [00:07<00:01, 450MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  81%|████████  | 2.83G/3.50G [00:07<00:01, 450MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  82%|████████▏ | 2.88G/3.50G [00:07<00:01, 422MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  84%|████████▍ | 2.94G/3.50G [00:07<00:01, 430MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  85%|████████▌ | 2.99G/3.50G [00:07<00:01, 438MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  87%|████████▋ | 3.04G/3.50G [00:07<00:01, 436MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  88%|████████▊ | 3.09G/3.50G [00:07<00:00, 432MB/s]\n",
      "#033[A\n",
      "Downloading (…)of-00002.safetensors:  90%|████████▉ | 3.15G/3.50G [00:07<00:00, 421MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  91%|█████████▏| 3.20G/3.50G [00:08<00:00, 401MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  93%|█████████▎| 3.24G/3.50G [00:08<00:00, 369MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  94%|█████████▍| 3.29G/3.50G [00:08<00:00, 387MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  96%|█████████▌| 3.34G/3.50G [00:08<00:00, 403MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  97%|█████████▋| 3.40G/3.50G [00:08<00:00, 403MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors:  99%|█████████▊| 3.45G/3.50G [00:08<00:00, 414MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:08<00:00, 421MB/s]#033[A\n",
      "Downloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:08<00:00, 397MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:32<00:00, 14.88s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [00:32<00:00, 16.18s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.35s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n",
      "Downloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 188/188 [00:00<00:00, 2.33MB/s]\n",
      "trainable params: 67,108,864 || all params: 3,567,521,792 || trainable%: 1.881105930466591\n",
      "0%|          | 0/1030 [00:00<?, ?it/s]\n",
      "0%|          | 1/1030 [00:13<3:56:38, 13.80s/it]\n",
      "0%|          | 2/1030 [00:27<3:54:28, 13.69s/it]\n",
      "0%|          | 3/1030 [00:41<3:53:39, 13.65s/it]\n",
      "0%|          | 4/1030 [00:54<3:53:09, 13.63s/it]\n",
      "0%|          | 5/1030 [01:08<3:52:44, 13.62s/it]\n",
      "1%|          | 6/1030 [01:21<3:52:26, 13.62s/it]\n",
      "1%|          | 7/1030 [01:35<3:52:09, 13.62s/it]\n",
      "1%|          | 8/1030 [01:49<3:51:53, 13.61s/it]\n",
      "1%|          | 9/1030 [02:02<3:51:38, 13.61s/it]\n",
      "1%|          | 10/1030 [02:16<3:51:24, 13.61s/it]\n",
      "{'loss': 1.3451, 'learning_rate': 9.902912621359223e-05, 'epoch': 0.01}\n",
      "1%|          | 10/1030 [02:16<3:51:24, 13.61s/it]\n",
      "1%|          | 11/1030 [02:29<3:51:11, 13.61s/it]\n",
      "1%|          | 12/1030 [02:43<3:50:56, 13.61s/it]\n",
      "1%|▏         | 13/1030 [02:57<3:50:43, 13.61s/it]\n",
      "1%|▏         | 14/1030 [03:10<3:50:29, 13.61s/it]\n",
      "1%|▏         | 15/1030 [03:24<3:50:15, 13.61s/it]\n",
      "2%|▏         | 16/1030 [03:37<3:50:01, 13.61s/it]\n",
      "2%|▏         | 17/1030 [03:51<3:49:47, 13.61s/it]\n",
      "2%|▏         | 18/1030 [04:05<3:49:35, 13.61s/it]\n",
      "2%|▏         | 19/1030 [04:18<3:49:21, 13.61s/it]\n",
      "2%|▏         | 20/1030 [04:32<3:49:08, 13.61s/it]\n",
      "{'loss': 1.0018, 'learning_rate': 9.805825242718448e-05, 'epoch': 0.02}\n",
      "2%|▏         | 20/1030 [04:32<3:49:08, 13.61s/it]\n",
      "2%|▏         | 21/1030 [04:46<3:48:54, 13.61s/it]\n",
      "2%|▏         | 22/1030 [04:59<3:48:40, 13.61s/it]\n",
      "2%|▏         | 23/1030 [05:13<3:48:27, 13.61s/it]\n",
      "2%|▏         | 24/1030 [05:26<3:48:13, 13.61s/it]\n",
      "2%|▏         | 25/1030 [05:40<3:48:00, 13.61s/it]\n",
      "3%|▎         | 26/1030 [05:54<3:47:46, 13.61s/it]\n",
      "3%|▎         | 27/1030 [06:07<3:47:32, 13.61s/it]\n",
      "3%|▎         | 28/1030 [06:21<3:47:18, 13.61s/it]\n",
      "3%|▎         | 29/1030 [06:34<3:47:05, 13.61s/it]\n",
      "3%|▎         | 30/1030 [06:48<3:46:52, 13.61s/it]\n",
      "{'loss': 0.7672, 'learning_rate': 9.70873786407767e-05, 'epoch': 0.03}\n",
      "3%|▎         | 30/1030 [06:48<3:46:52, 13.61s/it]\n",
      "3%|▎         | 31/1030 [07:02<3:46:38, 13.61s/it]\n",
      "3%|▎         | 32/1030 [07:15<3:46:25, 13.61s/it]\n",
      "3%|▎         | 33/1030 [07:29<3:46:10, 13.61s/it]\n",
      "3%|▎         | 34/1030 [07:42<3:45:57, 13.61s/it]\n",
      "3%|▎         | 35/1030 [07:56<3:45:44, 13.61s/it]\n",
      "3%|▎         | 36/1030 [08:10<3:45:30, 13.61s/it]\n",
      "4%|▎         | 37/1030 [08:23<3:45:17, 13.61s/it]\n",
      "4%|▎         | 38/1030 [08:37<3:45:02, 13.61s/it]\n",
      "4%|▍         | 39/1030 [08:51<3:44:48, 13.61s/it]\n",
      "4%|▍         | 40/1030 [09:04<3:44:35, 13.61s/it]\n",
      "{'loss': 0.6166, 'learning_rate': 9.611650485436893e-05, 'epoch': 0.04}\n",
      "4%|▍         | 40/1030 [09:04<3:44:35, 13.61s/it]\n",
      "4%|▍         | 41/1030 [09:18<3:44:22, 13.61s/it]\n",
      "4%|▍         | 42/1030 [09:31<3:44:08, 13.61s/it]\n",
      "4%|▍         | 43/1030 [09:45<3:43:54, 13.61s/it]\n",
      "4%|▍         | 44/1030 [09:59<3:43:40, 13.61s/it]\n",
      "4%|▍         | 45/1030 [10:12<3:43:27, 13.61s/it]\n",
      "4%|▍         | 46/1030 [10:26<3:43:13, 13.61s/it]\n",
      "5%|▍         | 47/1030 [10:39<3:42:59, 13.61s/it]\n",
      "5%|▍         | 48/1030 [10:53<3:42:45, 13.61s/it]\n",
      "5%|▍         | 49/1030 [11:07<3:42:32, 13.61s/it]\n",
      "5%|▍         | 50/1030 [11:20<3:42:19, 13.61s/it]\n",
      "{'loss': 0.5392, 'learning_rate': 9.514563106796118e-05, 'epoch': 0.05}\n",
      "5%|▍         | 50/1030 [11:20<3:42:19, 13.61s/it]\n",
      "5%|▍         | 51/1030 [11:34<3:42:06, 13.61s/it]\n",
      "5%|▌         | 52/1030 [11:47<3:41:52, 13.61s/it]\n",
      "5%|▌         | 53/1030 [12:01<3:41:37, 13.61s/it]\n",
      "5%|▌         | 54/1030 [12:15<3:41:24, 13.61s/it]\n",
      "5%|▌         | 55/1030 [12:28<3:41:11, 13.61s/it]\n",
      "5%|▌         | 56/1030 [12:42<3:40:57, 13.61s/it]\n",
      "6%|▌         | 57/1030 [12:56<3:40:44, 13.61s/it]\n",
      "6%|▌         | 58/1030 [13:09<3:40:30, 13.61s/it]\n",
      "6%|▌         | 59/1030 [13:23<3:40:16, 13.61s/it]\n",
      "6%|▌         | 60/1030 [13:36<3:40:03, 13.61s/it]\n",
      "{'loss': 0.5328, 'learning_rate': 9.417475728155341e-05, 'epoch': 0.06}\n",
      "6%|▌         | 60/1030 [13:36<3:40:03, 13.61s/it]\n",
      "6%|▌         | 61/1030 [13:50<3:39:50, 13.61s/it]\n",
      "6%|▌         | 62/1030 [14:04<3:39:37, 13.61s/it]\n",
      "6%|▌         | 63/1030 [14:17<3:39:23, 13.61s/it]\n",
      "6%|▌         | 64/1030 [14:31<3:39:08, 13.61s/it]\n",
      "6%|▋         | 65/1030 [14:44<3:38:55, 13.61s/it]\n",
      "6%|▋         | 66/1030 [14:58<3:38:42, 13.61s/it]\n",
      "7%|▋         | 67/1030 [15:12<3:38:28, 13.61s/it]\n",
      "7%|▋         | 68/1030 [15:25<3:38:13, 13.61s/it]\n",
      "7%|▋         | 69/1030 [15:39<3:37:59, 13.61s/it]\n",
      "7%|▋         | 70/1030 [15:52<3:37:45, 13.61s/it]\n",
      "{'loss': 0.4878, 'learning_rate': 9.320388349514564e-05, 'epoch': 0.07}\n",
      "7%|▋         | 70/1030 [15:52<3:37:45, 13.61s/it]\n",
      "7%|▋         | 71/1030 [16:06<3:37:31, 13.61s/it]\n",
      "7%|▋         | 72/1030 [16:20<3:37:17, 13.61s/it]\n",
      "7%|▋         | 73/1030 [16:33<3:37:03, 13.61s/it]\n",
      "7%|▋         | 74/1030 [16:47<3:36:49, 13.61s/it]\n",
      "7%|▋         | 75/1030 [17:01<3:36:35, 13.61s/it]\n",
      "7%|▋         | 76/1030 [17:14<3:36:21, 13.61s/it]\n",
      "7%|▋         | 77/1030 [17:28<3:36:08, 13.61s/it]\n",
      "8%|▊         | 78/1030 [17:41<3:35:55, 13.61s/it]\n",
      "8%|▊         | 79/1030 [17:55<3:35:41, 13.61s/it]\n",
      "8%|▊         | 80/1030 [18:09<3:35:28, 13.61s/it]\n",
      "{'loss': 0.488, 'learning_rate': 9.223300970873788e-05, 'epoch': 0.08}\n",
      "8%|▊         | 80/1030 [18:09<3:35:28, 13.61s/it]\n",
      "8%|▊         | 81/1030 [18:22<3:35:15, 13.61s/it]\n",
      "8%|▊         | 82/1030 [18:36<3:35:03, 13.61s/it]\n",
      "8%|▊         | 83/1030 [18:49<3:34:50, 13.61s/it]\n",
      "8%|▊         | 84/1030 [19:03<3:34:35, 13.61s/it]\n",
      "8%|▊         | 85/1030 [19:17<3:34:22, 13.61s/it]\n",
      "8%|▊         | 86/1030 [19:30<3:34:08, 13.61s/it]\n",
      "8%|▊         | 87/1030 [19:44<3:33:55, 13.61s/it]\n",
      "9%|▊         | 88/1030 [19:57<3:33:41, 13.61s/it]\n",
      "9%|▊         | 89/1030 [20:11<3:33:27, 13.61s/it]\n",
      "9%|▊         | 90/1030 [20:25<3:33:14, 13.61s/it]\n",
      "{'loss': 0.4824, 'learning_rate': 9.126213592233011e-05, 'epoch': 0.09}\n",
      "9%|▊         | 90/1030 [20:25<3:33:14, 13.61s/it]\n",
      "9%|▉         | 91/1030 [20:38<3:33:01, 13.61s/it]\n",
      "9%|▉         | 92/1030 [20:52<3:32:48, 13.61s/it]\n",
      "9%|▉         | 93/1030 [21:06<3:32:33, 13.61s/it]\n",
      "9%|▉         | 94/1030 [21:19<3:32:19, 13.61s/it]\n",
      "9%|▉         | 95/1030 [21:33<3:32:07, 13.61s/it]\n",
      "9%|▉         | 96/1030 [21:46<3:31:53, 13.61s/it]\n",
      "9%|▉         | 97/1030 [22:00<3:31:39, 13.61s/it]\n",
      "10%|▉         | 98/1030 [22:14<3:31:26, 13.61s/it]\n",
      "10%|▉         | 99/1030 [22:27<3:31:12, 13.61s/it]\n",
      "10%|▉         | 100/1030 [22:41<3:30:58, 13.61s/it]\n",
      "{'loss': 0.4681, 'learning_rate': 9.029126213592234e-05, 'epoch': 0.1}\n",
      "10%|▉         | 100/1030 [22:41<3:30:58, 13.61s/it]\n",
      "10%|▉         | 101/1030 [22:54<3:30:45, 13.61s/it]\n",
      "10%|▉         | 102/1030 [23:08<3:30:31, 13.61s/it]\n",
      "10%|█         | 103/1030 [23:22<3:30:18, 13.61s/it]\n",
      "10%|█         | 104/1030 [23:35<3:30:04, 13.61s/it]\n",
      "10%|█         | 105/1030 [23:49<3:29:50, 13.61s/it]\n",
      "10%|█         | 106/1030 [24:02<3:29:37, 13.61s/it]\n",
      "10%|█         | 107/1030 [24:16<3:29:24, 13.61s/it]\n",
      "10%|█         | 108/1030 [24:30<3:29:10, 13.61s/it]\n",
      "11%|█         | 109/1030 [24:43<3:28:56, 13.61s/it]\n",
      "11%|█         | 110/1030 [24:57<3:28:42, 13.61s/it]\n",
      "{'loss': 0.4716, 'learning_rate': 8.932038834951457e-05, 'epoch': 0.11}\n",
      "11%|█         | 110/1030 [24:57<3:28:42, 13.61s/it]\n",
      "11%|█         | 111/1030 [25:11<3:28:29, 13.61s/it]\n",
      "11%|█         | 112/1030 [25:24<3:28:15, 13.61s/it]\n",
      "11%|█         | 113/1030 [25:38<3:28:02, 13.61s/it]\n",
      "11%|█         | 114/1030 [25:51<3:27:48, 13.61s/it]\n",
      "11%|█         | 115/1030 [26:05<3:27:34, 13.61s/it]\n",
      "11%|█▏        | 116/1030 [26:19<3:27:21, 13.61s/it]\n",
      "11%|█▏        | 117/1030 [26:32<3:27:07, 13.61s/it]\n",
      "11%|█▏        | 118/1030 [26:46<3:26:54, 13.61s/it]\n",
      "12%|█▏        | 119/1030 [26:59<3:26:41, 13.61s/it]\n",
      "12%|█▏        | 120/1030 [27:13<3:26:27, 13.61s/it]\n",
      "{'loss': 0.4513, 'learning_rate': 8.834951456310681e-05, 'epoch': 0.12}\n",
      "12%|█▏        | 120/1030 [27:13<3:26:27, 13.61s/it]\n",
      "12%|█▏        | 121/1030 [27:27<3:26:13, 13.61s/it]\n",
      "12%|█▏        | 122/1030 [27:40<3:25:59, 13.61s/it]\n",
      "12%|█▏        | 123/1030 [27:54<3:25:46, 13.61s/it]\n",
      "12%|█▏        | 124/1030 [28:07<3:25:32, 13.61s/it]\n",
      "12%|█▏        | 125/1030 [28:21<3:25:17, 13.61s/it]\n",
      "12%|█▏        | 126/1030 [28:35<3:25:03, 13.61s/it]\n",
      "12%|█▏        | 127/1030 [28:48<3:24:49, 13.61s/it]\n",
      "12%|█▏        | 128/1030 [29:02<3:24:35, 13.61s/it]\n",
      "13%|█▎        | 129/1030 [29:16<3:24:21, 13.61s/it]\n",
      "13%|█▎        | 130/1030 [29:29<3:24:07, 13.61s/it]\n",
      "{'loss': 0.4509, 'learning_rate': 8.737864077669902e-05, 'epoch': 0.13}\n",
      "13%|█▎        | 130/1030 [29:29<3:24:07, 13.61s/it]\n",
      "13%|█▎        | 131/1030 [29:43<3:23:53, 13.61s/it]\n",
      "13%|█▎        | 132/1030 [29:56<3:23:40, 13.61s/it]\n",
      "13%|█▎        | 133/1030 [30:10<3:23:27, 13.61s/it]\n",
      "13%|█▎        | 134/1030 [30:24<3:23:14, 13.61s/it]\n",
      "13%|█▎        | 135/1030 [30:37<3:23:01, 13.61s/it]\n",
      "13%|█▎        | 136/1030 [30:51<3:22:48, 13.61s/it]\n",
      "13%|█▎        | 137/1030 [31:04<3:22:34, 13.61s/it]\n",
      "13%|█▎        | 138/1030 [31:18<3:22:21, 13.61s/it]\n",
      "13%|█▎        | 139/1030 [31:32<3:22:07, 13.61s/it]\n",
      "14%|█▎        | 140/1030 [31:45<3:21:54, 13.61s/it]\n",
      "{'loss': 0.439, 'learning_rate': 8.640776699029127e-05, 'epoch': 0.14}\n",
      "14%|█▎        | 140/1030 [31:45<3:21:54, 13.61s/it]\n",
      "14%|█▎        | 141/1030 [31:59<3:21:41, 13.61s/it]\n",
      "14%|█▍        | 142/1030 [32:12<3:21:28, 13.61s/it]\n",
      "14%|█▍        | 143/1030 [32:26<3:21:14, 13.61s/it]\n",
      "14%|█▍        | 144/1030 [32:40<3:21:01, 13.61s/it]\n",
      "14%|█▍        | 145/1030 [32:53<3:20:47, 13.61s/it]\n",
      "14%|█▍        | 146/1030 [33:07<3:20:33, 13.61s/it]\n",
      "14%|█▍        | 147/1030 [33:21<3:20:19, 13.61s/it]\n",
      "14%|█▍        | 148/1030 [33:34<3:20:06, 13.61s/it]\n",
      "14%|█▍        | 149/1030 [33:48<3:19:52, 13.61s/it]\n",
      "15%|█▍        | 150/1030 [34:01<3:19:39, 13.61s/it]\n",
      "{'loss': 0.4244, 'learning_rate': 8.54368932038835e-05, 'epoch': 0.15}\n",
      "15%|█▍        | 150/1030 [34:01<3:19:39, 13.61s/it]\n",
      "15%|█▍        | 151/1030 [34:15<3:19:25, 13.61s/it]\n",
      "15%|█▍        | 152/1030 [34:29<3:19:12, 13.61s/it]\n",
      "15%|█▍        | 153/1030 [34:42<3:18:58, 13.61s/it]\n",
      "15%|█▍        | 154/1030 [34:56<3:18:45, 13.61s/it]\n",
      "15%|█▌        | 155/1030 [35:09<3:18:31, 13.61s/it]\n",
      "15%|█▌        | 156/1030 [35:23<3:18:18, 13.61s/it]\n",
      "15%|█▌        | 157/1030 [35:37<3:18:04, 13.61s/it]\n",
      "15%|█▌        | 158/1030 [35:50<3:17:50, 13.61s/it]\n",
      "15%|█▌        | 159/1030 [36:04<3:17:37, 13.61s/it]\n",
      "16%|█▌        | 160/1030 [36:18<3:17:23, 13.61s/it]\n",
      "{'loss': 0.4271, 'learning_rate': 8.446601941747573e-05, 'epoch': 0.16}\n",
      "16%|█▌        | 160/1030 [36:18<3:17:23, 13.61s/it]\n",
      "16%|█▌        | 161/1030 [36:31<3:17:09, 13.61s/it]\n",
      "16%|█▌        | 162/1030 [36:45<3:16:56, 13.61s/it]\n",
      "16%|█▌        | 163/1030 [36:58<3:16:42, 13.61s/it]\n",
      "16%|█▌        | 164/1030 [37:12<3:16:28, 13.61s/it]\n",
      "16%|█▌        | 165/1030 [37:26<3:16:15, 13.61s/it]\n",
      "16%|█▌        | 166/1030 [37:39<3:16:01, 13.61s/it]\n",
      "16%|█▌        | 167/1030 [37:53<3:15:47, 13.61s/it]\n",
      "16%|█▋        | 168/1030 [38:06<3:15:34, 13.61s/it]\n",
      "16%|█▋        | 169/1030 [38:20<3:15:20, 13.61s/it]\n",
      "17%|█▋        | 170/1030 [38:34<3:15:07, 13.61s/it]\n",
      "{'loss': 0.4406, 'learning_rate': 8.349514563106797e-05, 'epoch': 0.17}\n",
      "17%|█▋        | 170/1030 [38:34<3:15:07, 13.61s/it]\n",
      "17%|█▋        | 171/1030 [38:47<3:14:53, 13.61s/it]\n",
      "17%|█▋        | 172/1030 [39:01<3:14:40, 13.61s/it]\n",
      "17%|█▋        | 173/1030 [39:14<3:14:26, 13.61s/it]\n",
      "\n",
      "2023-09-18 02:42:10 Stopping - Stopping the training job\n",
      "2023-09-18 02:42:10 Uploading - Uploading generated training model\n",
      "2023-09-18 02:42:10 Stopped - Training job stopped\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import telegram\n",
    "import asyncio\n",
    "\n",
    "api_key = '***REMOVED***'\n",
    "usr_id = '***REMOVED***'\n",
    "\n",
    "ver = \"v10\"\n",
    "training_input_path = f's3://{sess.default_bucket()}/datasets/goat/{ver}/training'\n",
    "validation_input_path = f's3://{sess.default_bucket()}/datasets/goat/{ver}/validation'\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path,\n",
    "        'validation': validation_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "try:\n",
    "        huggingface_estimator.fit(data, wait=True)\n",
    "        msg = 'SageMaker Training Finished!'\n",
    "except:\n",
    "        msg = 'SageMaker Training Finished with Error'\n",
    "finally:\n",
    "        bot = telegram.Bot(token=api_key)\n",
    "        async with bot:\n",
    "                await bot.send_message(chat_id=usr_id, text=msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.huggingface.model.HuggingFaceModel at 0x1541e4160>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "number_of_gpu = 1\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),\n",
    "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\",# Comment in to quantize\n",
    "# \n",
    "}\n",
    "image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04-v1.0\"\n",
    "\n",
    "huggingface_estimator.create_model(\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    env=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.telegram.org/bot***REMOVED***/getMe \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.telegram.org/bot***REMOVED***/sendMessage \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import telegram\n",
    "import asyncio\n",
    "\n",
    "api_key = '***REMOVED***'\n",
    "usr_id = '***REMOVED***'\n",
    "\n",
    "bot = telegram.Bot(token=api_key)\n",
    "async with bot:\n",
    "        await bot.send_message(chat_id=usr_id, text='SageMaker Training Finished!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example for LLaMA 13B, the SageMaker training job took `31728 seconds`, which is about `8.8 hours`. The ml.g5.4xlarge instance we used costs `$2.03 per hour` for on-demand usage. As a result, the total cost for training our fine-tuned LLaMa 2 model was only ~`$18`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps \n",
    "\n",
    "You can deploy your fine-tuned LLaMA model to a SageMaker endpoint and use it for inference. Check out the [Deploy Falcon 7B & 40B on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-llm) and [Securely deploy LLMs inside VPCs with Hugging Face and Amazon SageMaker](https://www.philschmid.de/sagemaker-llm-vpc) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Model from S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.huggingface.model.HuggingFaceModel object at 0x155132770>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "s3_model_uri = \"s3://sagemaker-ms-thesis-llm/models/goatV10-testData-withAutoInference-with-2023-09-02-11-38-37-525/output/model.tar.gz\"\n",
    "image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04-v1.0\"\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),\n",
    "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\",# Comment in to quantize\n",
    "# \n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    model_data=s3_model_uri,\n",
    "    env=config,\n",
    "    # source_dir=\"GOAT/code/\",\n",
    "    # entry_point=\"inference.py\"\n",
    ")\n",
    "\n",
    "print(llm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.tokens:Loading cached SSO token for slu-sso\n",
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2023-09-02-11-49-34-978\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2023-09-02-11-49-35-809\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2023-09-02-11-49-35-809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint huggingface-pytorch-tgi-inference-2023-09-02-11-49-35-809: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[39m=\u001b[39m llm_model\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m      2\u001b[0m   initial_instance_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m   instance_type\u001b[39m=\u001b[39;49minstance_type,\n\u001b[1;32m      4\u001b[0m   container_startup_health_check_timeout\u001b[39m=\u001b[39;49mhealth_check_timeout, \u001b[39m# 10 minutes to be able to load the model\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/projects/llm-data-driven-optimization/venv_310/lib/python3.10/site-packages/sagemaker/huggingface/model.py:313\u001b[0m, in \u001b[0;36mHuggingFaceModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     inference_tool \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mneuron\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m instance_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mml.inf1\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mneuronx\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_uri \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mserving_image_uri(\n\u001b[1;32m    308\u001b[0m         region_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mboto_session\u001b[39m.\u001b[39mregion_name,\n\u001b[1;32m    309\u001b[0m         instance_type\u001b[39m=\u001b[39minstance_type,\n\u001b[1;32m    310\u001b[0m         inference_tool\u001b[39m=\u001b[39minference_tool,\n\u001b[1;32m    311\u001b[0m     )\n\u001b[0;32m--> 313\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(HuggingFaceModel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m    314\u001b[0m     initial_instance_count,\n\u001b[1;32m    315\u001b[0m     instance_type,\n\u001b[1;32m    316\u001b[0m     serializer,\n\u001b[1;32m    317\u001b[0m     deserializer,\n\u001b[1;32m    318\u001b[0m     accelerator_type,\n\u001b[1;32m    319\u001b[0m     endpoint_name,\n\u001b[1;32m    320\u001b[0m     tags,\n\u001b[1;32m    321\u001b[0m     kms_key,\n\u001b[1;32m    322\u001b[0m     wait,\n\u001b[1;32m    323\u001b[0m     data_capture_config,\n\u001b[1;32m    324\u001b[0m     async_inference_config,\n\u001b[1;32m    325\u001b[0m     serverless_inference_config,\n\u001b[1;32m    326\u001b[0m     volume_size\u001b[39m=\u001b[39;49mvolume_size,\n\u001b[1;32m    327\u001b[0m     model_data_download_timeout\u001b[39m=\u001b[39;49mmodel_data_download_timeout,\n\u001b[1;32m    328\u001b[0m     container_startup_health_check_timeout\u001b[39m=\u001b[39;49mcontainer_startup_health_check_timeout,\n\u001b[1;32m    329\u001b[0m     inference_recommendation_id\u001b[39m=\u001b[39;49minference_recommendation_id,\n\u001b[1;32m    330\u001b[0m     explainer_config\u001b[39m=\u001b[39;49mexplainer_config,\n\u001b[1;32m    331\u001b[0m )\n",
      "File \u001b[0;32m~/projects/llm-data-driven-optimization/venv_310/lib/python3.10/site-packages/sagemaker/model.py:1430\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[39mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1428\u001b[0m     explainer_config_dict \u001b[39m=\u001b[39m explainer_config\u001b[39m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1430\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mendpoint_from_production_variants(\n\u001b[1;32m   1431\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint_name,\n\u001b[1;32m   1432\u001b[0m     production_variants\u001b[39m=\u001b[39;49m[production_variant],\n\u001b[1;32m   1433\u001b[0m     tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m   1434\u001b[0m     kms_key\u001b[39m=\u001b[39;49mkms_key,\n\u001b[1;32m   1435\u001b[0m     wait\u001b[39m=\u001b[39;49mwait,\n\u001b[1;32m   1436\u001b[0m     data_capture_config_dict\u001b[39m=\u001b[39;49mdata_capture_config_dict,\n\u001b[1;32m   1437\u001b[0m     explainer_config_dict\u001b[39m=\u001b[39;49mexplainer_config_dict,\n\u001b[1;32m   1438\u001b[0m     async_inference_config_dict\u001b[39m=\u001b[39;49masync_inference_config_dict,\n\u001b[1;32m   1439\u001b[0m )\n\u001b[1;32m   1441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1442\u001b[0m     predictor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m~/projects/llm-data-driven-optimization/venv_310/lib/python3.10/site-packages/sagemaker/session.py:4727\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict)\u001b[0m\n\u001b[1;32m   4724\u001b[0m LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCreating endpoint-config with name \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, name)\n\u001b[1;32m   4725\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mcreate_endpoint_config(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_options)\n\u001b[0;32m-> 4727\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_endpoint(\n\u001b[1;32m   4728\u001b[0m     endpoint_name\u001b[39m=\u001b[39;49mname, config_name\u001b[39m=\u001b[39;49mname, tags\u001b[39m=\u001b[39;49mendpoint_tags, wait\u001b[39m=\u001b[39;49mwait\n\u001b[1;32m   4729\u001b[0m )\n",
      "File \u001b[0;32m~/projects/llm-data-driven-optimization/venv_310/lib/python3.10/site-packages/sagemaker/session.py:4072\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   4068\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mcreate_endpoint(\n\u001b[1;32m   4069\u001b[0m     EndpointName\u001b[39m=\u001b[39mendpoint_name, EndpointConfigName\u001b[39m=\u001b[39mconfig_name, Tags\u001b[39m=\u001b[39mtags\n\u001b[1;32m   4070\u001b[0m )\n\u001b[1;32m   4071\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 4072\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait_for_endpoint(endpoint_name)\n\u001b[1;32m   4073\u001b[0m \u001b[39mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[0;32m~/projects/llm-data-driven-optimization/venv_310/lib/python3.10/site-packages/sagemaker/session.py:4424\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   4418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   4419\u001b[0m         \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   4420\u001b[0m             message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   4421\u001b[0m             allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mInService\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   4422\u001b[0m             actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   4423\u001b[0m         )\n\u001b[0;32m-> 4424\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   4425\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   4426\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mInService\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   4427\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   4428\u001b[0m     )\n\u001b[1;32m   4429\u001b[0m \u001b[39mreturn\u001b[39;00m desc\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-09-02-11-49-35-809: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.telegram.org/bot***REMOVED***/getMe \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.telegram.org/bot***REMOVED***/sendMessage \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import telegram\n",
    "import asyncio\n",
    "\n",
    "api_key = '***REMOVED***'\n",
    "usr_id = '***REMOVED***'\n",
    "\n",
    "bot = telegram.Bot(token=api_key)\n",
    "async with bot:\n",
    "        await bot.send_message(chat_id=usr_id, text='SageMaker Model Deploy Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Input Data for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple String Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = 0\n",
    "# tokenizer.padding_side = \"left\"  # Allow batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "   \"inputs\": \"<s>[INST] <<SYS>>\\n\\n You are a helpful math assistant\\n\\n<<SYS>>\\n\\n10 + 6 = \\n[/INST]<s>\"\n",
    "   # \"inputs\": \"\"\"You are a helpful AI assistant who responds to question simple and straightforward questions.\n",
    "   # Question: What is the Capital of California?\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"prompt\": \"[INST] What is the capital of CA? [/INST]\",\n",
    "    \"system_prompt\": \"You are a helpful assistant\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "{\n",
      "\"outputs\": \" <<SYS>>\\n\\n You are a helpful math assistant\\n\\n<<SYS>>\\n\\n10 + 6 = 16\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "  \"inputs\":  json.dumps(data),\n",
    "  \"parameters\": {\n",
    "    # \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    # \"stop\": [\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.huggingface.model.HuggingFaceModel object at 0x157278400>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "s3_model_uri = \"s3://sagemaker-ms-thesis-llm/checkpoints/huggingface-qlora-goatV8-testData\"\n",
    "image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04-v1.0\"\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),\n",
    "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\",# Comment in to quantize\n",
    "# \n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role,\n",
    "    image_uri=image_uri,\n",
    "    model_data=s3_model_uri,\n",
    "    env=config\n",
    ")\n",
    "\n",
    "print(llm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFaceModel' object has no attribute 'merge_and_unload'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# from peft import AutoPeftModelForCausalLM\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# model = AutoPeftModelForCausalLM.from_pretrained(\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m#         )  \u001b[39;00m\n\u001b[1;32m     10\u001b[0m         \u001b[39m# Merge LoRA and base model and save\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[39m=\u001b[39m llm_model\u001b[39m.\u001b[39;49mmerge_and_unload()\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(model)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFaceModel' object has no attribute 'merge_and_unload'"
     ]
    }
   ],
   "source": [
    "# from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#             output_dir,\n",
    "#             low_cpu_mem_usage=True,\n",
    "#             device_map=\"auto\", # Added from 28_train_llms_with_qlora example\n",
    "#             torch_dtype=torch.float16,\n",
    "#             trust_remote_code=True, # Added from 28_train_llms_with_qlora example\n",
    "#         )  \n",
    "        # Merge LoRA and base model and save\n",
    "model = llm_model.merge_and_unload()\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
