{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_PROFILE=dev-admin\n",
      "env: AWS_REGION=us-east-1\n",
      "env: HF_HOME=~/.cache/huggingface\n",
      "env: TOKENIZERS_PARALLELISM=fale\n"
     ]
    }
   ],
   "source": [
    "%env AWS_PROFILE=dev-admin\n",
    "%env AWS_REGION=us-east-1\n",
    "%env HF_HOME=~/.cache/huggingface\n",
    "%env TOKENIZERS_PARALLELISM=fale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMMAND_MODE': 'unix2003',\n",
       " 'HOME': '/Users/andrewbeiler',\n",
       " 'HOMEBREW_CELLAR': '/opt/homebrew/Cellar',\n",
       " 'HOMEBREW_PREFIX': '/opt/homebrew',\n",
       " 'HOMEBREW_REPOSITORY': '/opt/homebrew',\n",
       " 'INFOPATH': '/opt/homebrew/share/info:',\n",
       " 'LOGNAME': 'andrewbeiler',\n",
       " 'MANPATH': '/opt/homebrew/share/man::',\n",
       " 'MallocNanoZone': '0',\n",
       " 'OLDPWD': '/',\n",
       " 'ORIGINAL_XDG_CURRENT_DESKTOP': 'undefined',\n",
       " 'PATH': '/Users/andrewbeiler/projects/llm-data-driven-optimization/.venv/bin:/Library/Frameworks/Python.framework/Versions/3.8/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Applications/VMware Fusion.app/Contents/Public:/Library/Apple/usr/bin:/Library/Frameworks/Mono.framework/Versions/Current/Commands',\n",
       " 'PWD': '/',\n",
       " 'SHELL': '/bin/zsh',\n",
       " 'SHLVL': '1',\n",
       " 'SSH_AUTH_SOCK': '/private/tmp/com.apple.launchd.jeVzgmI8ek/Listeners',\n",
       " 'TMPDIR': '/var/folders/b7/s1kskgdx4txd_4ctzl8t3xpr0000gn/T/',\n",
       " 'USER': 'andrewbeiler',\n",
       " 'VSCODE_AMD_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess',\n",
       " 'VSCODE_CODE_CACHE_PATH': '/Users/andrewbeiler/Library/Application Support/Code/CachedData/7f329fe6c66b0f86ae1574c2911b681ad5a45d63',\n",
       " 'VSCODE_CRASH_REPORTER_PROCESS_TYPE': 'extensionHost',\n",
       " 'VSCODE_CWD': '/',\n",
       " 'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true',\n",
       " 'VSCODE_IPC_HOOK': '/Users/andrewbeiler/Library/Application Support/Code/1.77-main.sock',\n",
       " 'VSCODE_NLS_CONFIG': '{\"locale\":\"en-us\",\"osLocale\":\"en-us\",\"availableLanguages\":{},\"_languagePackSupport\":true}',\n",
       " 'VSCODE_PID': '8588',\n",
       " 'XPC_FLAGS': '0x0',\n",
       " 'XPC_SERVICE_NAME': '0',\n",
       " '_': '/Users/andrewbeiler/projects/llm-data-driven-optimization/.venv/bin/python',\n",
       " '__CFBundleIdentifier': 'com.microsoft.VSCode',\n",
       " '__CF_USER_TEXT_ENCODING': '0x1F5:0x0:0x0',\n",
       " 'ELECTRON_RUN_AS_NODE': '1',\n",
       " 'VSCODE_L10N_BUNDLE_LOCATION': '',\n",
       " 'PYTHONUNBUFFERED': '1',\n",
       " 'PYTHONIOENCODING': 'utf-8',\n",
       " 'VIRTUAL_ENV': '/Users/andrewbeiler/projects/llm-data-driven-optimization/.venv',\n",
       " 'PS1': '(.venv) ',\n",
       " 'VIRTUAL_ENV_PROMPT': '.venv',\n",
       " 'LC_CTYPE': 'UTF-8',\n",
       " 'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1',\n",
       " 'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'FORCE_COLOR': '1',\n",
       " 'CLICOLOR_FORCE': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://matplotlib_inline.backend_inline',\n",
       " 'AWS_PROFILE': 'dev-admin',\n",
       " 'AWS_REGION': 'us-east-1',\n",
       " 'HF_HOME': '~/.cache/huggingface',\n",
       " 'TOKENIZERS_PARALLELISM': '<hidden>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session_bucket = \"sagemaker-ms-thesis-llm\"\n",
    "role = \"arn:aws:iam::171706357329:role/service-role/SageMaker-ComputeAdmin\"\n",
    "boto_sess = boto3.Session(region_name=\"us-east-1\")\n",
    "sess = sagemaker.Session(boto_session=boto_sess, default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_goat_data(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    response = f\"### Answer\\n{sample['output']}\"\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewbeiler/projects/llm-data-driven-optimization/venv_310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Found cached dataset json (/Users/andrewbeiler/.cache/huggingface/datasets/tiedong___json/tiedong--goat-55b7467c033a1462/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 1746300\n",
      "Example Datapoint {'instruction': 'What is 279x86507440', 'output': '279 * 86507440 = 86507440 * (200 + 70 + 9) = 86507440 * 200 + 86507440 * 70 + 86507440 * 9 = 17301488000 + 6055520800 + 778566960 = 23357008800 + 778566960 = 24135575760', 'answer': '24135575760', 'input': '279 * 86507440'}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from prompter import Prompter\n",
    "\n",
    "from init import init_sagemaker\n",
    "\n",
    "\n",
    "prompter = Prompter()\n",
    "tokenizer = None\n",
    "\n",
    "cutoff_len = 512\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "dataset = load_dataset(\"tiedong/goat\", split=\"train\")\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(f\"Example Datapoint {dataset[randrange(len(dataset))]}\")\n",
    "# print(math.ceil(len(dataset)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "what is 3117112959 + 335153928218\n",
      "\n",
      "### Answer\n",
      "3117112959 + 335153928218 = 338271041177\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_goat_data(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_sample size: 10\n",
      "{'instruction': 'Calculate the answer to 66231613267717+60882409809.', 'output': '66231613267717 + 60882409809 = 66292495677526', 'answer': '66292495677526', 'input': '66231613267717 + 60882409809'}\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10#0000\n",
    "n_shards = math.ceil(len(dataset)/n_samples)\n",
    "dataset_sampled = dataset.shard(n_shards,2)\n",
    "print(f\"dataset_sample size: {len(dataset_sampled)}\")\n",
    "print(dataset_sampled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewbeiler/projects/llm-data-driven-optimization/venv_310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Data\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = 0\n",
    "# tokenizer.padding_side = \"left\"  # Allow batched inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "361034298956504/4\n",
      "\n",
      "### Answer\n",
      "361034298956504 / 4 = 90258574739126</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_goat_data(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset_sampled = dataset_sampled.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset_sampled[randint(0, len(dataset_sampled))][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '### Instruction\\n8017+5797033\\n\\n### Answer\\n8017 + 5797033 = 5805050</s>'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_sampled[randint(0, len(dataset_sampled))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/andrewbeiler/.cache/huggingface/datasets/tiedong___json/tiedong--goat-55b7467c033a1462/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-554ceb99aab038aa.arrow\n",
      "Loading cached processed dataset at /Users/andrewbeiler/.cache/huggingface/datasets/tiedong___json/tiedong--goat-55b7467c033a1462/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-adf884cf483af4d2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1\n"
     ]
    }
   ],
   "source": [
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    batch_chunk_length = 0\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "    print(batch_total_length)\n",
    "    print(chunk_length)\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "    else:\n",
    "        batch_chunk_length = batch_total_length\n",
    "    \n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset_sampled.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset_sampled.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "{'input_ids': [1, 835, 2799, 4080, 13, 27065, 403, 278, 1234, 304, 29871, 29953, 29953, 29906, 29941, 29896, 29953, 29896, 29941, 29906, 29953, 29955, 29955, 29896, 29955, 29974, 29953, 29900, 29947, 29947, 29906, 29946, 29900, 29929, 29947, 29900, 29929, 29889, 13, 13, 2277, 29937, 673, 13, 29953, 29953, 29906, 29941, 29896, 29953, 29896, 29941, 29906, 29953, 29955, 29955, 29896, 29955, 718, 29871, 29953, 29900, 29947, 29947, 29906, 29946, 29900, 29929, 29947, 29900, 29929, 353, 29871, 29953, 29953, 29906, 29929, 29906, 29946, 29929, 29945, 29953, 29955, 29955, 29945, 29906, 29953, 2, 1, 835, 2799, 4080, 13, 29947, 29900, 29896, 29955, 29974, 29945, 29955, 29929, 29955, 29900, 29941, 29941, 13, 13, 2277, 29937, 673, 13, 29947, 29900, 29896, 29955, 718, 29871, 29945, 29955, 29929, 29955, 29900, 29941, 29941, 353, 29871, 29945, 29947, 29900, 29945, 29900, 29945, 29900, 2, 1, 835, 2799, 4080, 13, 29896, 29929, 29906, 29946, 29953, 29896, 29900, 29906, 29929, 29955, 29945, 29947, 29941, 29947, 29947, 448, 29871, 29946, 29896, 29929, 29900, 29953, 29953, 29900, 29945, 29941, 29947, 29906, 29955, 29900, 29947, 29953, 13, 13, 2277, 29937, 673, 13, 29896, 29929, 29906, 29946, 29953, 29896, 29900, 29906, 29929, 29955, 29945, 29947, 29941, 29947, 29947, 448, 29871, 29946, 29896, 29929, 29900, 29953, 29953, 29900, 29945, 29941, 29947, 29906, 29955, 29900, 29947, 29953, 353, 448, 29906, 29906, 29953, 29953, 29900, 29945, 29900, 29906, 29946, 29900, 29953, 29947, 29953, 29929, 29947, 2, 1, 835, 2799, 4080, 13, 12148, 8147, 29871, 29953, 29941, 29929, 29929, 29899, 29953, 29947, 29941, 29945, 322, 2649, 592, 278, 16259, 21957, 29889, 13, 13, 2277, 29937, 673, 13, 29953, 29941, 29929, 29929, 448, 29871, 29953, 29947, 29941, 29945, 353, 448, 29946, 29941, 29953, 2, 1, 835, 2799, 4080, 13, 29947, 29930, 29945, 29953, 29941, 29929, 29900, 29941, 29896, 29945, 29900, 29945, 29941, 29906, 29945, 13, 13, 2277, 29937, 673, 13, 29947, 334, 29871, 29945, 29953, 29941, 29929, 29900, 29941, 29896, 29945, 29900, 29945, 29941, 29906, 29945, 353, 29871, 29946, 29945, 29896, 29896, 29906, 29906, 29945, 29906, 29900, 29946, 29906, 29953, 29900, 29900, 2, 1, 835, 2799, 4080, 13, 2929, 345, 29871, 29941, 29900, 29900, 29916, 29953, 29906, 29946, 29900, 29947, 29906, 13, 13, 2277, 29937, 673, 13, 29941, 29900, 29900, 334, 29871, 29953, 29906, 29946, 29900, 29947, 29906, 353, 29871, 29896, 29947, 29955, 29906, 29906, 29946, 29953, 29900, 29900, 2, 1, 835, 2799, 4080, 13, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 29916, 29941, 29953, 29945, 29953, 353, 13, 13, 2277, 29937, 673, 13, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 29871, 29941, 29953, 29945, 29953, 353, 29871, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 313, 29941, 29900, 29900, 29900, 718, 29871, 29953, 29900, 29900, 718, 29871, 29945, 29900, 718, 29871, 29953, 29897, 353, 29871, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 29871, 29941, 29900, 29900, 29900, 718, 29871, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 29871, 29953, 29900, 29900, 718, 29871, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 29871, 29945, 29900, 718, 29871, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 29871, 29953, 353, 29871, 29953, 29946, 29945, 29896, 29947, 29941, 29929, 29929, 29900, 29900, 29900, 718, 29871, 29896, 29906, 29929, 29900, 29941, 29953, 29955, 29929, 29947, 29900, 29900, 718, 29871, 29896, 29900, 29955, 29945, 29941, 29900, 29953, 29953, 29945, 29900, 718, 29871, 29896, 29906, 29929, 29900, 29941, 29953, 29955, 29929, 29947, 353, 29871, 29955, 29955, 29946, 29906, 29906, 29900, 29955, 29947, 29947, 29900, 29900, 718, 29871, 29896, 29900, 29955, 29945, 29941, 29900, 29953, 29953, 29945, 29900, 718, 29871, 29896, 29906, 29929, 29900, 29941, 29953, 29955, 29929, 29947, 353, 29871, 29955, 29947, 29946, 29929, 29955, 29941, 29947, 29945, 29946, 29945, 29900, 718, 29871, 29896, 29906, 29929, 29900, 29941, 29953, 29955, 29929, 29947, 353, 29871, 29955, 29947, 29953, 29906, 29953, 29946, 29906, 29906, 29906, 29946, 29947, 2, 1, 835, 2799, 4080, 13, 29941, 29953, 29896, 29900, 29941, 29946, 29906, 29929, 29947, 29929, 29945, 29953, 29945, 29900, 29946, 29914, 29946, 13, 13, 2277, 29937, 673, 13, 29941, 29953, 29896, 29900, 29941, 29946, 29906, 29929, 29947, 29929, 29945, 29953, 29945, 29900, 29946, 847, 29871, 29946, 353, 29871, 29929, 29900, 29906, 29945, 29947, 29945, 29955, 29946, 29955, 29941, 29929, 29896, 29906, 29953, 2, 1, 835, 2799, 4080, 13, 29941, 29945, 29929, 29955, 29929, 29941, 29946, 29914, 29955, 13, 13, 2277, 29937, 673, 13, 29941, 29945, 29929, 29955, 29929, 29941, 29946, 847, 29871, 29955, 353, 29871, 29945, 29896, 29941, 29929, 29929, 29900, 390, 29871, 29946, 2, 1, 835, 2799, 4080, 13, 6039, 2408, 278, 995, 363, 29871, 29941, 29900, 29955, 29946, 847, 29871, 29929, 29953, 29947, 29889, 13, 13, 2277, 29937, 673, 13, 29941, 29900, 29955, 29946, 448, 29871, 29929, 29953, 29947, 334, 29871, 29941, 353, 29871, 29941, 29900, 29955, 29946, 448, 29871, 29906, 29929, 29900, 29946, 353, 29871, 29896, 29955, 29900, 13, 8439, 1079, 29892, 29871, 29941, 29900, 29955, 29946, 847, 29871, 29929, 29953, 29947, 353, 29871, 29941, 390, 29871, 29896, 29955, 29900, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1, 835, 2799, 4080, 13, 27065, 403, 278, 1234, 304, 29871, 29953, 29953, 29906, 29941, 29896, 29953, 29896, 29941, 29906, 29953, 29955, 29955, 29896, 29955, 29974, 29953, 29900, 29947, 29947, 29906, 29946, 29900, 29929, 29947, 29900, 29929, 29889, 13, 13, 2277, 29937, 673, 13, 29953, 29953, 29906, 29941, 29896, 29953, 29896, 29941, 29906, 29953, 29955, 29955, 29896, 29955, 718, 29871, 29953, 29900, 29947, 29947, 29906, 29946, 29900, 29929, 29947, 29900, 29929, 353, 29871, 29953, 29953, 29906, 29929, 29906, 29946, 29929, 29945, 29953, 29955, 29955, 29945, 29906, 29953, 2, 1, 835, 2799, 4080, 13, 29947, 29900, 29896, 29955, 29974, 29945, 29955, 29929, 29955, 29900, 29941, 29941, 13, 13, 2277, 29937, 673, 13, 29947, 29900, 29896, 29955, 718, 29871, 29945, 29955, 29929, 29955, 29900, 29941, 29941, 353, 29871, 29945, 29947, 29900, 29945, 29900, 29945, 29900, 2, 1, 835, 2799, 4080, 13, 29896, 29929, 29906, 29946, 29953, 29896, 29900, 29906, 29929, 29955, 29945, 29947, 29941, 29947, 29947, 448, 29871, 29946, 29896, 29929, 29900, 29953, 29953, 29900, 29945, 29941, 29947, 29906, 29955, 29900, 29947, 29953, 13, 13, 2277, 29937, 673, 13, 29896, 29929, 29906, 29946, 29953, 29896, 29900, 29906, 29929, 29955, 29945, 29947, 29941, 29947, 29947, 448, 29871, 29946, 29896, 29929, 29900, 29953, 29953, 29900, 29945, 29941, 29947, 29906, 29955, 29900, 29947, 29953, 353, 448, 29906, 29906, 29953, 29953, 29900, 29945, 29900, 29906, 29946, 29900, 29953, 29947, 29953, 29929, 29947, 2, 1, 835, 2799, 4080, 13, 12148, 8147, 29871, 29953, 29941, 29929, 29929, 29899, 29953, 29947, 29941, 29945, 322, 2649, 592, 278, 16259, 21957, 29889, 13, 13, 2277, 29937, 673, 13, 29953, 29941, 29929, 29929, 448, 29871, 29953, 29947, 29941, 29945, 353, 448, 29946, 29941, 29953, 2, 1, 835, 2799, 4080, 13, 29947, 29930, 29945, 29953, 29941, 29929, 29900, 29941, 29896, 29945, 29900, 29945, 29941, 29906, 29945, 13, 13, 2277, 29937, 673, 13, 29947, 334, 29871, 29945, 29953, 29941, 29929, 29900, 29941, 29896, 29945, 29900, 29945, 29941, 29906, 29945, 353, 29871, 29946, 29945, 29896, 29896, 29906, 29906, 29945, 29906, 29900, 29946, 29906, 29953, 29900, 29900, 2, 1, 835, 2799, 4080, 13, 2929, 345, 29871, 29941, 29900, 29900, 29916, 29953, 29906, 29946, 29900, 29947, 29906, 13, 13, 2277, 29937, 673, 13, 29941, 29900, 29900, 334, 29871, 29953, 29906, 29946, 29900, 29947, 29906, 353, 29871, 29896, 29947, 29955, 29906, 29906, 29946, 29953, 29900, 29900, 2, 1, 835, 2799, 4080, 13, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 29916, 29941, 29953, 29945, 29953, 353, 13, 13, 2277, 29937, 673, 13, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 29871, 29941, 29953, 29945, 29953, 353, 29871, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 313, 29941, 29900, 29900, 29900, 718, 29871, 29953, 29900, 29900, 718, 29871, 29945, 29900, 718, 29871, 29953, 29897, 353, 29871, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 29871, 29941, 29900, 29900, 29900, 718, 29871, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 29871, 29953, 29900, 29900, 718, 29871, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 29871, 29945, 29900, 718, 29871, 29906, 29896, 29945, 29900, 29953, 29896, 29941, 29941, 334, 29871, 29953, 353, 29871, 29953, 29946, 29945, 29896, 29947, 29941, 29929, 29929, 29900, 29900, 29900, 718, 29871, 29896, 29906, 29929, 29900, 29941, 29953, 29955, 29929, 29947, 29900, 29900, 718, 29871, 29896, 29900, 29955, 29945, 29941, 29900, 29953, 29953, 29945, 29900, 718, 29871, 29896, 29906, 29929, 29900, 29941, 29953, 29955, 29929, 29947, 353, 29871, 29955, 29955, 29946, 29906, 29906, 29900, 29955, 29947, 29947, 29900, 29900, 718, 29871, 29896, 29900, 29955, 29945, 29941, 29900, 29953, 29953, 29945, 29900, 718, 29871, 29896, 29906, 29929, 29900, 29941, 29953, 29955, 29929, 29947, 353, 29871, 29955, 29947, 29946, 29929, 29955, 29941, 29947, 29945, 29946, 29945, 29900, 718, 29871, 29896, 29906, 29929, 29900, 29941, 29953, 29955, 29929, 29947, 353, 29871, 29955, 29947, 29953, 29906, 29953, 29946, 29906, 29906, 29906, 29946, 29947, 2, 1, 835, 2799, 4080, 13, 29941, 29953, 29896, 29900, 29941, 29946, 29906, 29929, 29947, 29929, 29945, 29953, 29945, 29900, 29946, 29914, 29946, 13, 13, 2277, 29937, 673, 13, 29941, 29953, 29896, 29900, 29941, 29946, 29906, 29929, 29947, 29929, 29945, 29953, 29945, 29900, 29946, 847, 29871, 29946, 353, 29871, 29929, 29900, 29906, 29945, 29947, 29945, 29955, 29946, 29955, 29941, 29929, 29896, 29906, 29953, 2, 1, 835, 2799, 4080, 13, 29941, 29945, 29929, 29955, 29929, 29941, 29946, 29914, 29955, 13, 13, 2277, 29937, 673, 13, 29941, 29945, 29929, 29955, 29929, 29941, 29946, 847, 29871, 29955, 353, 29871, 29945, 29896, 29941, 29929, 29929, 29900, 390, 29871, 29946, 2, 1, 835, 2799, 4080, 13, 6039, 2408, 278, 995, 363, 29871, 29941, 29900, 29955, 29946, 847, 29871, 29929, 29953, 29947, 29889, 13, 13, 2277, 29937, 673, 13, 29941, 29900, 29955, 29946, 448, 29871, 29929, 29953, 29947, 334, 29871, 29941, 353, 29871, 29941, 29900, 29955, 29946, 448, 29871, 29906, 29929, 29900, 29946, 353, 29871, 29896, 29955, 29900, 13, 8439, 1079, 29892, 29871, 29941, 29900, 29955, 29946, 847, 29871, 29929, 29953, 29947, 353, 29871, 29941, 390, 29871, 29896, 29955, 29900, 2]}\n"
     ]
    }
   ],
   "source": [
    "print(type(lm_dataset))\n",
    "print(lm_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97abee122a74902b2b7a95177da9f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset to: s3://sagemaker-ms-thesis-llm/datasets/goat/v7\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "ver = \"v7\"\n",
    "dataset_name = \"goat\"\n",
    "training_input_path = f's3://{sagemaker_session_bucket}/datasets/{dataset_name}/{ver}'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
